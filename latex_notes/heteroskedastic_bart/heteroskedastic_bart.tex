\include{preamble}

\title{Some notes on the Metropolis-Hastings Implementation}

\date{}

\newcommand{\treet}[1]{\text{\PHplaneTree}_{#1}}

\begin{document}
\maketitle

\section*{Heteroskedastic BART Introduction}

We take vanilla BART and update the last step of the Gibbs sampler to include a sampling an estimate for the variance for each observation, $\sigsq_1, \ldots, \sigsq_n$.

\beqn
\treet{1} &~|~& R_1, \sigsq_1, \ldots, \sigsq_n \\
M_1 &~|~& \treet{1}, R_1, \sigsq_1, \ldots, \sigsq_n \\
\vdots && \\
\treet{m} &~|~& R_m, \sigsq_1, \ldots, \sigsq_n \\
M_m &~|~& \treet{m}, R_m, \sigsq_1, \ldots, \sigsq_n \\
\sigsq_1 &~|~& E \\
\vdots && \\
\sigsq_n &~|~& E \\
\eeqn

\subsection*{Prior on the variance}

We keep the same prior on each of the observation's variances:

\beqn
\sigsq_1, \ldots, \sigsq_n \iid \invgammanot{\overtwo{\nu}}{\overtwo{\nu\lambda}}
\eeqn

We may have to rethink the above, but it seems like a good start. Once again we pick $\nu = 3$ and we pick $\lambda$ so that 90\% of the prior's density is below $s^2$, our estimate from the data which is either just the sample variance, or the RMSE squared from a linear model. 

We also can have individual priors:

\beqn
\sigsq_1 &\sim& \invgammanot{\overtwo{\nu}}{\overtwo{\nu\lambda_1}} \\
\vdots && \\
\sigsq_n &\sim& \invgammanot{\overtwo{\nu}}{\overtwo{\nu\lambda_n}} \\
\eeqn

where $\lambda_i$ is picked based on a weighted least squares residual.

\subsection*{Changes in Gibbs Sampler for $M$}

The other step that has to change is sampling the leaf $\mu$'s. Remember that $M_t ~|~ \treet{1}, R_1, \sigsq_1, \ldots, \sigsq_n$ is actually the sampling for all leaves where each leaf is considered independent:

\beqn
\mu_{t1} &~|~&  \treet{t}, R_{t_1}, \sigsq_1, \ldots, \sigsq_n \\
\vdots && \\
\mu_{tb_t} &~|~&  \treet{t}, R_{t_{b_t}}, \sigsq_1, \ldots, \sigsq_n \\
\eeqn

The subscripts on the $R$ term indicates we only consider the data that falls into the leaf. So remember that the prior on the leaf value was normal, and the likelihood was assumed normal as well. The only thing that changes is we now have a different variance estimate for each observation. 

We derive the correct posterior distribution below. We drop the subscripts just for convenience since it will be the same for each of the above and denote $k$ as the number of data records that fell into this leaf. $\braces{\sigsq_1, \ldots, \sigsq_k} \subset \braces{\sigsq_1, \ldots, \sigsq_n}$ for the data in this leaf as well:

\beqn
\cprob{\mu}{R, \sigsq_1, \ldots, \sigsq_k} &\propto& \cprob{R}{\mu, \sigsq_1, \ldots, \sigsq_n} \cprob{\mu}{\sigsq_1, \ldots, \sigsq_k} \\
&=& \multnormnot{k}{\mu\onevec}{\underbrace{\threebythreemat{\sigsq_1}{}{}{}{\ddots}{}{}{}{\sigsq_k}}_{\D}} \normnot{0}{\sigsq_\mu} \\
&\vdots& \\
&=& \normnot{\frac{\dfrac{k^2 \Rbar}{\sum_{i=1}^k \sigsq_i}}{\doneover{\sigsq_\mu} + \dfrac{k^2}{\sum_{i=1}^k \sigsq_i}}}{\oneover{\doneover{\sigsq_\mu} + \dfrac{k^2}{\sum_{i=1}^k \sigsq_i}}}
\eeqn

\subsection*{Changes in Gibbs Sampler for $\sigsq_i$}

In vanilla BART, the posterior of $\sigsq$ was:

\beqn
\sigsq ~|~ e_1, \ldots, e_n \sim \invgammanot{\overtwo{\nu + n}}{\overtwo{\lambda\nu + \sum_{i=1}^n e_i^2}} 
\eeqn

Now, we have to sample each $\sigsq_1, \ldots, \sigsq_n$. We estimate each of them based on only one residual:

\beqn
\sigsq_i ~|~ e_i \sim \invgammanot{\overtwo{\nu + 1}}{\overtwo{\lambda\nu + e_i^2}}
\eeqn


\subsection*{Changes in Likelihood for $R$}

In vanilla BART, we had:

\beqn
&& \frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} \\
&=& \frac{\myint{\mu}{\reals}{}{\cprob{\RLlonetonlL}{\mu, \sigsq}\prob{\mu}} ~~ \myint{\mu}{\reals}{}{\cprob{\RRlonetonlR}{\mu, \sigsq}\prob{\mu}}}{\myint{\mu}{\reals}{}{\cprob{\Rlonetonl}{\mu, \sigsq}\prob{\mu}}}  \\
&\vdots& \\
&=& \sqrt{\frac{\sigsq \parens{\sigsq + n_\ell \sigsqmu}}{\parens{\sigsq + n_{\ell_L} \sigsqmu}\parens{\sigsq + n_{\ell_R} \sigsqmu}}}~\exp{\frac{\sigsq_\mu}{2\sigsq} \parens{\frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell_L, i}}}{\sigsq + n_{\ell_L}\sigsq_\mu} + \frac{\squared{\sum_{i=1}^{n_{\ell_R}} R_{\ell_R, i}}}{\sigsq + n_{\ell_R}\sigsq_\mu} - \frac{\squared{\sum_{i=1}^{n_{\ell}} R_{\ell, i}}}{\sigsq + n_\ell \sigsq_\mu}}} \\
\eeqn

We cannot use this expression as before since $\sigsq$ is now non-constant. Instead we have to use:

\small
\beqn
&& \frac{\cprob{\R}{T^*, \sigsq_1, \ldots, \sigsq_n}}{\cprob{\R}{T, \sigsq_1, \ldots, \sigsq_n}} \\
&=& \frac{\myint{\mu}{\reals}{}{\cprob{\RLlonetonlL}{\mu, \sigsq_1, \ldots, \sigsq_n}\prob{\mu}} ~~ \myint{\mu}{\reals}{}{\cprob{\RRlonetonlR}{\mu, \sigsq_1, \ldots, \sigsq_n}\prob{\mu}}}{\myint{\mu}{\reals}{}{\cprob{\Rlonetonl}{\mu, \sigsq_1, \ldots, \sigsq_n}\prob{\mu}}} 
\eeqn
\normalsize

Thus, we have to figure out the below. I'm going to drop the subscripts for convenience:

\beqn
&& \myint{\mu}{\reals}{}{\cprob{\R}{\mu, \sigsq_1, \ldots, \sigsq_n}\prob{\mu}}  \\
&=& \myint{\mu}{\reals}{}{\oneover{\tothepow{2\pi}{\overtwo{n}} \abss{\D}^{-\half}}\exp{-\half\transpose{\R - \mu\onevec} \D^{-1} (\R - \mu\onevec)} ~\oneoversqrt{2\pi\sigsq_\mu} \exp{-\oneover{2\sigsq_\mu} \musq}}~ \\
&=& \myint{\mu}{\reals}{}{\parens{\prod_{i=1}^{n} \oneoversqrt{2\pi\sigsq_i} \exp{-\oneover{2\sigsq_i} \squared{R_i - \mu}}} ~\oneoversqrt{2\pi\sigsq_\mu} \exp{-\oneover{2\sigsq_\mu} \musq}}~ \\
&=& \oneover{\tothepow{2\pi}{\overtwo{n} + 1}\sqrt{\sigsqmu \prod_{i=1}^{n} \sigsq_i}}\myint{\mu}{\reals}{}{\exp{-\half \parens{\musq + \sumonen{i}{\squared{R_i - \mu}}} }}\\
&=& \oneover{\tothepow{2\pi}{\overtwo{n} + 1}\sqrt{\sigsqmu \prod_{i=1}^{n} \sigsq_i}}\myint{\mu}{\reals}{}{\exp{-\half \parens{\frac{\musq}{\sigsq_\mu} + \sumonen{i}{\frac{R_i^2}{\sigsq_i} - \frac{2\mu R_i}{\sigsq_i} + \frac{\musq}{\sigsq_i}}} }}\\
&=& \oneover{\tothepow{2\pi}{\overtwo{n} + 1}\sqrt{\sigsqmu \prod_{i=1}^{n} \sigsq_i}}\myint{\mu}{\reals}{}{\exp{-\half \parens{\frac{\musq}{\sigsq_\mu} + \sumonen{i}{\frac{R_i^2}{\sigsq_i}} - 2\mu \sumonen{i}{\frac{R_i}}{\sigsq_i} + \musq\sumonen{i}{\frac{1}{\sigsq_i}}} }}\\
&=& \oneover{\tothepow{2\pi}{\overtwo{n} + 1}\sqrt{\sigsqmu \prod_{i=1}^{n} \sigsq_i}} \exp{-\half \sumonen{i}{\frac{R_i^2}{\sigsq_i}}}\myint{\mu}{\reals}{}{\exp{-\half \parens{\frac{\musq}{\sigsq_\mu} - 2\mu \sumonen{i}{\frac{R_i}}{\sigsq_i} + \musq\sumonen{i}{\frac{1}{\sigsq_i}}} }}\\
\eeqn

Now we can solve with Mathematica:

\begin{figure}[htp]
\centering
\includegraphics[width=5.5in]{mathematica.jpg}
\end{figure}
\FloatBarrier

Thus we arrive at:

\beqn
&=& \oneover{\tothepow{2\pi}{\overtwo{n} + 1}\sqrt{\sigsqmu \prod_{i=1}^{n} \sigsq_i}} \exp{-\half \sumonen{i}{\frac{R_i^2}{\sigsq_i}}} \sqrt{\frac{2\pi}{\oneover{\sigsq_\mu} + \displaystyle \sum_{i=1}^n \oneover{\sigsq_i}}} \exp{\frac{\sigsq_\mu \squared{\displaystyle \sum_{i=1}^n \frac{R_i}{\sigsq_i}}}{2 + 2\sigsq_\mu \displaystyle \sum_{i=1}^n \oneover{\sigsq_i}}}\\
&=& \oneover{\tothepow{2\pi}{\overtwo{n}}\sqrt{\parens{\displaystyle \prod_{i=1}^{n} \sigsq_i} \parens{1 + \sigsq_\mu \displaystyle \sum_{i=1}^n \oneover{\sigsq_i}}}} \exp{-\half \sumonen{i}{\frac{R_i^2}{\sigsq_i}}}  \exp{\overtwo{\sigsq_\mu} \parens{\frac{\squared{\displaystyle \sum_{i=1}^n \frac{R_i}{\sigsq_i}}}{1 + \sigsq_\mu \displaystyle \sum_{i=1}^n \oneover{\sigsq_i}}}}\\
\eeqn

Now we can solve for the likelihood ratio:

\beqn
\frac{\cprob{\R}{T^*, \sigsq_1, \ldots, \sigsq_n}}{\cprob{\R}{T, \sigsq_1, \ldots, \sigsq_n}} = \frac{\cprob{\R_L}{\sigsq_1, \ldots, \sigsq_n} \cprob{\R_R}{\sigsq}}{\cprob{\R}{\sigsq_1, \ldots, \sigsq_n}} \\
\eeqn

Note that there's many things that can cancel above. The sum of the $R_i^2 / \sigsq_i$'s would cancel numerator-denominator since we're adding the same set\footnote{the aggregate of the left leaf values $\cup$ the aggregate of the right leaf values has to equal the parent's values}, the product of the $\sigsq_i$'s would cancel since we're multiplying over the same set, the $2\pi$'s will cancel as well leaving us with:

\begin{changemargin}{-20px}{0px}
\small
\beqn
&=& \sqrt{\frac{\parens{1 + \sigsq_\mu \displaystyle \sum_{i_L=1}^{n_L} \oneover{\sigsq_i}}\parens{1 + \sigsq_\mu \displaystyle \sum_{i_R=1}^{n_R} \oneover{\sigsq_i}}}{1 + \sigsq_\mu \displaystyle \sum_{i=1}^n \oneover{\sigsq_i}}}  \exp{\overtwo{\sigsq_\mu} \parens{\frac{\squared{\displaystyle \sum_{i_L=1}^{n_L} \frac{R_i}{\sigsq_i}}}{1 + \sigsq_\mu \displaystyle \sum_{i_L=1}^{n_L} \oneover{\sigsq_i}} + \frac{\squared{\displaystyle \sum_{i_R=1}^{n_R} \frac{R_i}{\sigsq_i}}}{1 + \sigsq_\mu \displaystyle \sum_{i_R=1}^{n_R} \oneover{\sigsq_i}} -  \frac{\squared{\displaystyle \sum_{i=1}^{n} \frac{R_i}{\sigsq_i}}}{1 + \sigsq_\mu \displaystyle \sum_{i=1}^{n} \oneover{\sigsq_i}}}}
\eeqn
\normalsize
\end{changemargin}

In log form this becomes:

\inblue{
\beqn
&& \half \parens{\natlog{1 + \sigsq_\mu \displaystyle \sum_{i_L=1}^{n_L} \oneover{\sigsq_i}} + \natlog{1 + \sigsq_\mu \displaystyle \sum_{i_R=1}^{n_R} \oneover{\sigsq_i}} - \natlog{1 + \sigsq_\mu \displaystyle \sum_{i=1}^{n} + \oneover{\sigsq_i}}} + \\
&& \overtwo{\sigsq_\mu} \parens{\frac{\squared{\displaystyle \sum_{i_L=1}^{n_L} \frac{R_i}{\sigsq_i}}}{1 + \sigsq_\mu \displaystyle \sum_{i_L=1}^{n_L} \oneover{\sigsq_i}} + \frac{\squared{\displaystyle \sum_{i_R=1}^{n_R} \frac{R_i}{\sigsq_i}}}{1 + \sigsq_\mu \displaystyle \sum_{i_R=1}^{n_R} \oneover{\sigsq_i}} -  \frac{\squared{\displaystyle \sum_{i=1}^{n} \frac{R_i}{\sigsq_i}}}{1 + \sigsq_\mu \displaystyle \sum_{i=1}^{n} \oneover{\sigsq_i}}}
\eeqn
}

\end{document}