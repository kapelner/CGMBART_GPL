\include{preamble}

\title{Some notes on the Metropolis-Hastings Implementation}

\date{}

\begin{document}
\maketitle

\section*{MCMC for BART}

According to Gelman p.291, the Metropolis-Hastings algorithm differs from the Metropolis algorithm because you need to consider the ratio of ratios:

\beqn
r = \frac{\dfrac{\cprob{\theta^*}{Y}}{J_t(\theta^* ~|~ \theta^{t-1})}}{\dfrac{\cprob{\theta^{t-1}}{Y}}{J_t(\theta^{t-1} ~|~ \theta^*)}} = \frac{J_t(\theta^{t-1} ~|~ \theta^*)}{J_t(\theta^* ~|~ \theta^{t-1})}\frac{\prob{\theta^* ~|~ Y}}{\prob{\theta^{t-1} ~|~ Y}}
\eeqn

Where we accept if a random draw from a uniform is less than the ratio above \ie $X \sim \uniform{0}{1} \leadsto x < r$. \\

The parameters of interest in our case are the new tree (which is created from one of three types of proposals), which we denote $T^*$, and the original tree, denoted by $T$. For jump notation which is $J$, we denote this just using the regular probability symbol (to be boring). We denote the residual left over from $Y$, the vector of random variables, as $\R := \bracks{\Roneton}^\top$. We denote $\sigsq$ as the random variable estimating the homoskedastic noise in the model which is sampled after all the trees are sampled. We do not notate all the hyperparameters to avoid notational messiness. The M-H ratio then becomes:

\beqn
r = \frac{\prob{T^* \rightarrow T}}{\prob{T \rightarrow T^*}} \frac{\cprob{T^*}{\R, \sigsq}}{\cprob{T}{\R, \sigsq}}
\eeqn

%We then use Bayes Rule to obtain the following which I partition form. We partition the ratio into three interpretable terms:

%\beqn
%r = \underbrace{\frac{\cprob{T}{T^*}}{\cprob{T^*}{T}}}_{\text{transitioning ratio}} \times \overbrace{\frac{\cprob{R}{T^*, \sigsq}}{\cprob{R}{T, \sigsq}}}^{\text{proportional likelihood ratio}} \times \underbrace{\frac{\prob{T^*}}{\prob{T}}}_{\text{tree structure ratio}}
%\eeqn

My goal is to come up with an exact way of calculating $r$ for all possible tree proposals.\\

Throughout this document, we use the following notation:

\begin{itemize}
\item $H$ ---  the collection of \textit{all} nodes in tree $T$, not just the terminal nodes
\item $\eta$ --- a node $\in H$.
\item $d_\eta$ --- the depth of the $\eta$th node. The root node is defined as having depth 0, its first child has depth 1, etc.
\item $b$ --- the number of terminal nodes / leaves in the tree $T$. These are the nodes that can potentially be ``grown.'' For example, the number of terminal nodes in the $T^*$ tree is $b+1$ if $T$ was grown and $b-1$ if $T$ was pruned.
\item $w_2$ --- the number of 2nd generation internal nodes in tree $T$ \ie the number of nodes who only have two children. These are the nodes that can potentially be ``pruned.''
\item $w_2^*$ --- the number of 2nd generation internal nodes in tree $T^*$. This can be equal to $w$ or $w+1$ so it has to be recalculated. Draw a few pictures of trees and grow steps and you'll see why this unfortunate fact is so.
%\item $w$ --- the number of internal nodes regardless of generation (even the root is included in this count).
\item  $\ell$ --- the index of the terminal node which we've ``picked'' to grow from it is a number $\in \braces{1, \ldots, b}$.
\item $\ell_L$ and $\ell_R$ --- represent the new left and right node indices in tree $T^*$ which are ``grown'' from the $\ell$th node of tree $T$
\end{itemize}

\subsection*{Likelihood Calculation}

It is imperative that we can calculate $\cprob{T}{\R, \sigsq}$ for the calculation of $r$. Let's analyze carefully what it means to get the likelihood of a tree.

First, note that the likelihood for $T$ given the data is not defined in our model, so we use Bayes Rule to obtain something that is tractable in our model:

\beqn
\cprob{T}{\R, \sigsq} = \frac{\cprob{\R}{T, \sigsq} \cprob{T}{\sigsq}}{\cprob{\R}{\sigsq}}
\eeqn

What's in the numerator? The likelihood of the data given the tree and the variance times the probability of the tree given the variance. The probability of the tree is based on probabilities of splits and rules and is not dependent on the variance, $\cprob{T}{\sigsq} = \prob{T}$, so we can already simplify to:

\beqn
\cprob{T}{\R, \sigsq} = \frac{\cprob{\R}{T, \sigsq} \prob{T}}{\cprob{\R}{\sigsq}}
\eeqn

What about the likelihood given the tree? The only reason you need the $T$ information is so we know which $R$ values fall in which of the leaves:

\beqn
\cprob{\Roneton}{T, \sigsq} = \prod_{\ell=1}^{b} \cprob{\Rlonetonl}{\sigsq}
\eeqn

The r.h.s is the likelihoods of each leaf separately which is \textit{not} dependent on $T$ anymore. We can multiply the likelihoods of each leaf because we assume the leaves are independent. The $R_\ell$'s are the data in the $\ell$th leaf and there is $n_\ell$ of them, the portion of $n$ in the leaf where $n = \sum_{\ell=1}^b n_\ell$.

Let's look at the likelihood of a single leaf more carefully. We know that if we knew the mean at the leaf, which we denote $\mu_\ell$, we would have:

\beqn
\Rlonetonl | \mu_\ell, \sigsq ~\iid~ \normnot{\mu}{\sigsq}
\eeqn

This means that if we can margin out $\mu$, we're in good shape. Recall that we put a prior on the average value of $\mu_\ell \sim \normnot{0}{\sigsq_\mu}$ and thus:

\beqn
\cprob{\Rlonetonl}{\sigsq} = \int_\reals \cprob{\Rlonetonl}{\mu_\ell, \sigsq} \prob{\mu_\ell; \sigsq_\mu} d\mu_\ell
\eeqn

Let's take a more careful look at the non-margined leaf-likelihood expression and recall from basic mathematical statistics the result that the sample average, $\Rbar_\ell$, is a sufficient statistic for $\mu_\ell$ via the factorization theorem:



\beqn
\prob{\Rlonetonl | \mu_\ell, \sigsq} &=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \mu_\ell}} \\
&=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \parens{n_\ell \squared{\Rbar_\ell - \mu_\ell} + \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}}} \\
&=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\frac{\sigsq}{n_\ell}} \parens{ \squared{\Rbar_\ell - \mu_\ell} + \oneover{n_\ell} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}}} \\
&=& \underbrace{\oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}}  ~\exp{\oneover{n_\ell} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}}}_h ~\underbrace{\exp{-\oneover{2\frac{\sigsq}{n_\ell}} \parens{ \squared{\Rbar_\ell - \mu_\ell}}}}_g \\
\eeqn



So now we margin and notice that the integral is the definition of a convolution:

\begin{changemargin}{-0.5in}{0in}
\beqn
\cprob{\Rlonetonl}{\sigsq} &=& \int_\reals \prob{\Rlonetonl | \mu_\ell, \sigsq} \prob{\mu_\ell; \sigsq_\mu} d\mu_\ell \\
&=& h \sqrt{2\pi\frac{\sigsq}{n_\ell}}\int_\reals \sqrt{\frac{1}{2\pi\frac{\sigsq}{n_\ell}}}\exp{-\oneover{2\frac{\sigsq}{n_\ell}} \parens{ \squared{\Rbar_\ell - \mu_\ell}}} \oneoversqrt{2\pi\sigsq_\mu} ~\exp{-\oneover{2\sigsq_\mu} \musq}  d\mu_\ell \\
&=& h \sqrt{2\pi\frac{\sigsq}{n_\ell}} \normnot{0}{\frac{\sigsq}{n_\ell}} \star \normnot{0}{\sigsq_\mu} \\
&=& h \sqrt{2\pi\frac{\sigsq}{n_\ell}} \normnot{0}{\frac{\sigsq}{n_\ell} + \sigsq_\mu} \\
&=& h \sqrt{2\pi\frac{\sigsq}{n_\ell}} \oneoversqrt{2\pi \parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}} ~\exp{-\oneover{2 \parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}} \Rbar_\ell^2}\\
&=& h \sqrt{\frac{\sigsq}{\sigsq n_\ell + \sigsq_\mu}}~\exp{-\oneover{2 \parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}} \Rbar_\ell^2}\\
\eeqn
\end{changemargin}

%The problem here is that $g$ is not a likelihood. So I don't know how to do that margining trick with $\normnot{0}{\overn{\sigsq}} \star \normnot{0}{\sigsq_\mu} = \normnot{0}{\overn{\sigsq} + \sigsq_\mu}$. So I think I have to do the integral manually using Mathematica:
%
%\beqn
%\int_\reals g(\Rbar_\ell ~|~ \mu, \sigsq) \prob{\mu; \sigsq_\mu} d\mu &=& \int_\reals \exp{-\oneover{2\sigsq} \parens{-2n_\ell \mu \Rbar_\ell + n_\ell \musq}} \oneoversqrt{2\pi\sigsq_\mu} \exp{-\oneover{2\sigsq_\mu} \musq} d\mu \\
%&=& \oneoversqrt{\sigsq_\mu \parens{\frac{n_\ell}{\sigsq} + 1}} \exp{\frac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\sigsq\parens{\sigsq + n_\ell \sigsq_\mu}}} \\
%\Rightarrow \cprob{\Rlonetonl}{\sigsq} &\propto& \oneoversqrt{\sigsq_\mu \parens{\frac{n_\ell}{\sigsq} + 1}} \exp{\frac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\sigsq\parens{\sigsq + n_\ell \sigsq_\mu}}} \\
%\eeqn

%\begin{figure}
%\begin{center}
%\includegraphics[width=6in]{integrate_out_mu2.eps}
%\end{center}
%\end{figure}
%\FloatBarrier

%And this result does not appear to be equal to $\normnot{0}{\overn{\sigsq} + \sigsq_\mu}$. \\

Let's finish this section off by returning to the probability of the tree which was:

\beqn
\cprob{T}{\R, \sigsq} = \frac{\cprob{\R}{T, \sigsq} \prob{T}}{\cprob{\R}{\sigsq}}
\eeqn

What about the denominator --- the probability of the data? The probability of the data is weighted over every possible tree configuration:

\beqn
\cprob{\R}{\sigsq} = \int_{T \in \mathcal{T}} \cprob{\R}{T, \sigsq} \prob{T} dT
\eeqn

and removing the dependency on $T$ becomes:

\beqn
\cprob{\R}{\sigsq} = \int_{T \in \mathcal{T}} \prod_{\ell=1}^{b_T} \cprob{\Rlonetonl}{\sigsq} \prob{T} dT
\eeqn

which of course is arrived at via the margining out of the means of each leaf:

\beqn
\cprob{\R}{\sigsq} = \int_{T \in \mathcal{T}} \parens{\prod_{\ell=1}^{b_T} \int_\reals \cprob{\Rlonetonl}{\mu, \sigsq} \prob{\mu; \sigsq_\mu} d\mu} \prob{T} dT
\eeqn

The point being is that this quantity is the same for all data $\R$. This is useful since we're creating ratios where we're using the same data, and this quantity will cancel.

Now let's look at the ratio of the likelihoods which is what we care about for the calculation of $r$. I identify three pieces which we will use for the next couple of sections:

\beqn
r &=& \frac{\prob{T^* \rightarrow T}}{\prob{T \rightarrow T^*}} \underbrace{\frac{\cprob{T^*}{\R, \sigsq}}{\cprob{T}{\R, \sigsq}}} \\
&& \quad\quad\quad\quad~~ \frac{\dfrac{\cprob{\R}{T^*, \sigsq} \prob{T^*}}{\cancel{\cprob{\R}{\sigsq}}}}{\dfrac{\cprob{\R}{T, \sigsq} \prob{T}}{\cancel{\cprob{\R}{\sigsq}}}} \\
&=& \underbrace{\frac{\prob{T^* \rightarrow T}}{\prob{T \rightarrow T^*}}}_{\text{transition ratio}} \times \overbrace{\frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}}}^{\text{proportional likelihood ratio}} \times \underbrace{\frac{\prob{T^*}}{\prob{T}}}_{\text{tree structure ratio}}
\eeqn

\subsection*{Grow Proposal}

Let's pretend we're transitioning from $T \rightarrow T^*$ using a GROW step and let's analyze each of the above expressions one-by-one.

\subsubsection*{Transition Ratio}

So $\prob{T \rightarrow T^*}$ means the probability of transitioning from $T$ into the new tree proposal $T^*$. This would have to be equal to the following:

\beqn
\prob{T \rightarrow T^*} &=& \prob{\text{GROW}} \prob{\text{selecting the $\ell$th node to grow from}} \times \\
&& \prob{\text{selecting the $j$th attribute to split on}} \prob{\text{selecting the $i$th value to split on}}
\eeqn

We're picking from on of the terminal nodes, and then we're picking an attribute and split point, this becomes:

\beqn
\cprob{T^*}{T} &=& \prob{\text{GROW}} \oneover{b} \oneover{p_{adj}} \oneover{n_{adj}}
\eeqn

Now, $p_{adj}$ is the number of predictors left available to split on. This is \textit{from the perspective} of the $\ell$th node in tree $T$. Why would this be less than $p$? Because if you look up into the node's lineage, you may have already used all available split values for some attributes. Those would no longer be available to split from.

Then, $n_{adj}$ is the number of split values left available considering we picked attribute $j$. We can obtain this by looking at the node's lineage for any splits on $j$ and then taking the minimum of those split values, and then finding the subset of the design matrix whose values are less than that minimum for column $j$.\\

So now $\prob{T^* \rightarrow T}$ is the probability of transitioning from the new tree back to the old tree which would be:

\beqn
\prob{T^* \rightarrow T} &=& \prob{\text{PRUNE}} \prob{\text{selecting the $\ell$th node to prune from}} \\
&=& \prob{\text{PRUNE}}\oneover{w_2^*}
\eeqn

Thus, the transition ratio will be:

\beqn
\frac{\prob{T^* \rightarrow T}}{\prob{T \rightarrow T^*}} = \frac{\prob{\text{PRUNE}}\oneover{w_2^*}}{\prob{\text{GROW}}\oneover{b} \oneover{p_{adj}} \oneover{n_{adj}}} = \frac{\prob{\text{PRUNE}}}{\prob{\text{GROW}}} \frac{b ~ p_{adj} ~ n_{adj}}{w_2^*}
\eeqn

Why don't the probabilities of prune and grow cancel? Well, under the case where we cannot grow anymore (if we use all split variables), the probability of growth will be 0. Thus, those steps \textit{cannot} be considered since the ratio would be undefined. As long as they can be considered, that ratio will cancel.

\subsubsection*{Proportional Likelihood Ratio}

$\cprob{\R}{T^*, \sigsq}$ represents the likelihood of all the responses (adjusted by the other trees) to wind up in the nodes they've wound up in. As we've shown in the previous section that the likelihood for any node is:

%\beqn
%\cprob{\Rlonetonl}{\sigsq} = h(\Rlonetonl ~|~ \sigsq) \oneoversqrt{\sigsq_\mu \parens{\frac{n_\ell}{\sigsq} + 1}} \exp{\frac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\parens{\sigma^4 + n_\ell\sigsq \sigsq_\mu}}}
%\eeqn


\beqn
\cprob{\Rlonetonl}{\sigsq} = h \sqrt{\frac{\sigsq}{\sigsq n_\ell + \sigsq_\mu}}~\exp{-\oneover{2 \parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}} \Rbar_\ell^2}\\
\eeqn

Since the likelihoods are solely determined by the terminal nodes, the proposal tree differs from the original tree by only the $\ell$th node in the original becoming the $\ell_L$ and $\ell_R$ nodes in the proposal. Hence, the proportional likelihood ratio becomes only:

\beqn
\frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} &=& h_L \sqrt{\frac{\sigsq}{\sigsq n_{\ell_L} + \sigsq_\mu}}~\exp{-\oneover{2 \parens{\frac{\sigsq}{n_{\ell_L}} + \sigsq_\mu}} \Rbar_{\ell_L}^2} \\
&& \times h_R \sqrt{\frac{\sigsq}{\sigsq n_{\ell_R} + \sigsq_\mu}}~\exp{-\oneover{2 \parens{\frac{\sigsq}{n_{\ell_R}} + \sigsq_\mu}} \Rbar_{\ell_R}^2} \times \\
&& \inverse{h \sqrt{\frac{\sigsq}{\sigsq n_\ell + \sigsq_\mu}}~\exp{-\oneover{2 \parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}} \Rbar_\ell^2}}
\eeqn

%\beqn
%\frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} &=& h\parens{\RLlonetonlL ~|~ \sigsq} \oneoversqrt{\sigsq_\mu \parens{\frac{n_{\ell_L}}{\sigsq} + 1}} ~~\exp{\frac{n_{\ell_L}^2 \Rbar_{\ell_L}^2 \sigsq_\mu}{2\parens{\sigma^4 + n_{\ell_L}\sigsq \sigsq_\mu}}} \\
%&& ~~\times h\parens{\RRlonetonlR ~|~ \sigsq} \oneoversqrt{\sigsq_\mu \parens{\frac{n_{\ell_R}}{\sigsq} + 1}} ~~\exp{\frac{n_{\ell_R}^2 \Rbar_{\ell_R}^2 \sigsq_\mu}{2\parens{\sigma^4 + n_{\ell_R}\sigsq \sigsq_\mu}}} \\
%&& ~~\times \inverse{h\parens{\Rlonetonl ~|~ \sigsq} \oneoversqrt{\sigsq_\mu \parens{\frac{n_\ell}{\sigsq} + 1}} ~~\exp{\dfrac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\parens{\sigma^4 + n_\ell\sigsq \sigsq_\mu}}}} \\
%\eeqn

Note that the ratio of the $h$ functions do \textit{not} cancel:

\beqn
&& \frac{h_L h_R}{h} = \frac{h\parens{\RLlonetonlL ~|~ \sigsq}h\parens{\RRlonetonlR ~|~ \sigsq}}{h\parens{\Rlonetonl ~|~ \sigsq}} \\
&=& \frac{\oneover{\tothepow{2\pi\sigsq}{n_{\ell_L} / 2}}  ~\exp{\oneover{n_{\ell_L}} \sum_{i=1}^{n_{\ell_L}} \squared{R_{\ell_{L,i}} - \Rbar_{\ell_L}}} \oneover{\tothepow{2\pi\sigsq}{n_{\ell_R} / 2}}  ~\exp{\oneover{n_{\ell_R}} \sum_{i=1}^{n_{\ell_R}} \squared{R_{\ell_{R,i}} - \Rbar_{\ell_R}}}}{\oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}}  ~\exp{\oneover{n_\ell} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}}} \\
&=& \frac{\exp{\oneover{n_{\ell_L}} \sum_{i=1}^{n_{\ell_L}} \squared{R_{\ell_{L,i}} - \Rbar_{\ell_L}} + \oneover{n_{\ell_R}} \sum_{i=1}^{n_{\ell_R}} \squared{R_{\ell_{R,i}} - \Rbar_{\ell_R}}}}{\exp{\oneover{n_\ell} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}}} \eqncomment{since $n_\ell = n_{\ell_R} + n_{\ell_L}$}
\eeqn



Hence, we cannot really simplify the above expression for the proportional likelihood ratio.

\subsubsection*{Alternative Calculation}

Let's factorize the $\iid$ normal likelihood like the following:

\beqn
\prob{\Rlonetonl | \mu, \sigsq} &=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \mu}} \\
&=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} R_{\ell_i}^2} \underbrace{\exp{-\oneover{2\sigsq} \parens{-2n_\ell \mu \Rbar_\ell + n_\ell \musq}}}_g \\
%&=& h(\Rlonetonl ~|~ \sigsq) \cdot g(\Rbar_\ell ~|~ \mu, \sigsq) \\
\eeqn

Now, let's try to build the ratio:

\small
\begin{changemargin}{-1in}{0in}
\beqn
&& \frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} \\
&=& \frac{\cprob{\RLlonetonlL}{\sigsq} \cprob{\RRlonetonlR}{\sigsq}}{\cprob{\Rlonetonl}{\sigsq}} \\
&=& \frac{\myint{\mu}{\reals}{}{\cprob{\RLlonetonlL}{\mu, \sigsq}\prob{\mu}} ~~ \myint{\mu}{\reals}{}{\cprob{\RRlonetonlR}{\mu, \sigsq}\prob{\mu}}}{\myint{\mu}{\reals}{}{\cprob{\Rlonetonl}{\mu, \sigsq}\prob{\mu}}} \\
&=& \frac{\oneover{\tothepow{2\pi\sigsq}{n_{\ell_L} / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_{\ell_L}} R_{{\ell_L}_i}^2} \parens{\myint{\mu}{\reals}{}{g_{\ell_L} \prob{\mu}}}~ \oneover{\tothepow{2\pi\sigsq}{n_{\ell_R} / 2}}  ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_{\ell_R}} R_{{\ell_R}_i}^2} \parens{\myint{\mu}{\reals}{}{g_{\ell_R} \prob{\mu}}}}{\oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} R_{\ell_i}^2} \parens{\myint{\mu}{\reals}{}{g_{\ell} \prob{\mu}}}} \\
&=& \frac{\cancel{\oneover{\tothepow{2\pi\sigsq}{n_{\ell_L} / 2}}} ~\cancel{\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_{\ell_L}} R_{{\ell_L}_i}^2}} \parens{\myint{\mu}{\reals}{}{g_{\ell_L} \prob{\mu}}}~ \cancel{\oneover{\tothepow{2\pi\sigsq}{n_{\ell_R} / 2}}}  ~\cancel{\exp{-\oneover{2\sigsq}} \sum_{i=1}^{n_{\ell_R}} R_{{\ell_R}_i}^2} \parens{\myint{\mu}{\reals}{}{g_{\ell_R} \prob{\mu}}}}{\cancel{\oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}}} ~\cancel{\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} R_{\ell_i}^2}} \parens{\myint{\mu}{\reals}{}{g_{\ell} \prob{\mu}}}} \\
&=& \frac{\myint{\mu}{\reals}{}{g_{\ell_L} \prob{\mu}} \myint{\mu}{\reals}{}{g_{\ell_R} \prob{\mu}}}{\myint{\mu}{\reals}{}{g_{\ell} \prob{\mu}}} \\
&=& \frac{\myint{\mu}{\reals}{}{g_{\ell_L} \cancel{\oneoversqrt{2\pi\sigsqmu}} \exp{-\oneover{2\sigsqmu} \musq}} \myint{\mu}{\reals}{}{g_{\ell_R} \oneoversqrt{2\pi\sigsq_\mu} \exp{-\oneover{2\sigsqmu} \musq}}}{\myint{\mu}{\reals}{}{g_{\ell} \cancel{\oneoversqrt{2\pi\sigsq_\mu}} \exp{-\oneover{2\sigsqmu} \musq}}} \\
&=& \oneoversqrt{2\pi\sigsq_\mu} \frac{\myint{\mu}{\reals}{}{g_{\ell_L} \exp{-\oneover{2\sigsqmu} \musq}} \myint{\mu}{\reals}{}{g_{\ell_R}  \exp{-\oneover{2\sigsqmu} \musq}}}{\myint{\mu}{\reals}{}{g_{\ell} \exp{-\oneover{2\sigsqmu} \musq}}} \\
&=& \oneoversqrt{2\pi\sigsq_\mu} \frac{\myint{\mu}{\reals}{}{\exp{-\oneover{2\sigsq} \parens{-2n_{\ell_L} \mu \Rbar_{\ell_L} + n_{\ell_L} \musq}} \exp{-\oneover{2\sigsqmu} \musq}} \myint{\mu}{\reals}{}{\exp{-\oneover{2\sigsq} \parens{-2n_{\ell_L} \mu \Rbar_{\ell_L} + n_{\ell_L} \musq}}  \exp{-\oneover{2\sigsqmu} \musq}}}{\underbrace{\myint{\mu}{\reals}{}{\exp{-\oneover{2\sigsq} \parens{-2n_\ell \mu \Rbar_\ell + n_\ell \musq}} \exp{-\oneover{2\sigsqmu} \musq}}}} \\
\eeqn
\end{changemargin}
\normalsize

Let's evaluate the underbraced integral using Mathematica:

%The problem here is that $g$ is not a likelihood. So I don't know how to do that margining trick with $\normnot{0}{\overn{\sigsq}} \star \normnot{0}{\sigsq_\mu} = \normnot{0}{\overn{\sigsq} + \sigsq_\mu}$. So I think I have to do the integral manually using Mathematica:

\beqn
\int_\reals g(\Rbar_\ell ~|~ \mu, \sigsq) \prob{\mu; \sigsq_\mu} d\mu &=& \int_\reals \exp{-\oneover{2\sigsq} \parens{-2n_\ell \mu \Rbar_\ell + n_\ell \musq}} \oneoversqrt{2\pi\sigsq_\mu} \exp{-\oneover{2\sigsq_\mu} \musq} d\mu \\
&=& \sqrt{\frac{2\pi}{\frac{n_\ell}{\sigsq} + \oneover{\sigsqmu}}} \exp{\frac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\sigsq\parens{\sigsq + n_\ell \sigsq_\mu}}} \\
%\Rightarrow \cprob{\Rlonetonl}{\sigsq} &\propto& \oneoversqrt{\sigsq_\mu \parens{\frac{n_\ell}{\sigsq} + 1}} \exp{\frac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\sigsq\parens{\sigsq + n_\ell \sigsq_\mu}}} \\
\eeqn

So now we have:

\small
\begin{changemargin}{-1in}{0in}
\beqn
&& \frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} \\
&=& \oneoversqrt{2\pi\sigsq_\mu} \frac{\sqrt{\dfrac{2\pi}{\frac{n_{\ell_L}}{\sigsq} + \oneover{\sigsqmu}}} \exp{\dfrac{n_{\ell_L}^2 \Rbar_{\ell_L}^2 \sigsq_\mu}{2\sigsq\parens{\sigsq + n_{\ell_L} \sigsq_\mu}}} \sqrt{\dfrac{2\pi}{\frac{n_{\ell_R}}{\sigsq} + \oneover{\sigsqmu}}} \exp{\dfrac{n_{\ell_R}^2 \Rbar_{\ell_R}^2 \sigsq_\mu}{2\sigsq\parens{\sigsq + n_{\ell_R} \sigsq_\mu}}}}{\sqrt{\dfrac{2\pi}{\frac{n_\ell}{\sigsq} + \oneover{\sigsqmu}}} \exp{\dfrac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\sigsq\parens{\sigsq + n_\ell \sigsq_\mu}}}} \\
&=& \sqrt{\frac{\frac{n_\ell}{\sigsq} + \oneover{\sigsqmu}}{\sigsqmu \parens{\frac{n_{\ell_L}}{\sigsq} + \oneover{\sigsqmu}}\parens{\frac{n_{\ell_R}}{\sigsq} + \oneover{\sigsqmu}}}} ~~\exp{\frac{\sigsq_\mu}{2\sigsq} \parens{\frac{n_{\ell_L}^2 \Rbar_{\ell_L}^2}{\sigsq + n_{\ell_L}\sigsq_\mu} + \frac{n_{\ell_R}^2 \Rbar_{\ell_R}^2}{\sigsq + n_{\ell_R}\sigsq_\mu} - \frac{n_\ell^2 \Rbar_\ell^2}{\sigsq + n_\ell \sigsq_\mu}}} \\
&=& \sqrt{\frac{\sigsq \parens{\sigsq + n_\ell \sigsqmu}}{\parens{\sigsq + n_{\ell_L} \sigsqmu}\parens{\sigsq + n_{\ell_R} \sigsqmu}}}~~\exp{\frac{\sigsq_\mu}{2\sigsq} \parens{\frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell_L, i}}}{\sigsq + n_{\ell_L}\sigsq_\mu} + \frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell_R, i}}}{\sigsq + n_{\ell_R}\sigsq_\mu} - \frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell, i}}}{\sigsq + n_\ell \sigsq_\mu}}} \\
\eeqn
\end{changemargin}
\normalsize

%\begin{figure}
%\begin{center}
%\includegraphics[width=6in]{integrate_out_mu2.eps}
%\end{center}
%\end{figure}
%\FloatBarrier


%\beqn
%&& \frac{h\parens{\RLlonetonlL ~|~ \sigsq}h\parens{\RRlonetonlR ~|~ \sigsq}}{h\parens{\Rlonetonl ~|~ \sigsq}} \\
%&=& \frac{\doneover{\tothepow{2\pi\sigsq}{n_{\ell_L} / 2}} ~\exp{-\doneover{2\sigsq} \sum_{i=1}^{n_{\ell_L}} R_{\ell_i}^2} \doneover{\tothepow{2\pi\sigsq}{n_{\ell_R} / 2}} ~\exp{-\doneover{2\sigsq} \sum_{i=1}^{n_{\ell_R}} R_{\ell_i}^2}}{\doneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\doneover{2\sigsq} \sum_{i=1}^{n_\ell} R_{\ell_i}^2}} \\
%&=& \frac{\doneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\doneover{2\sigsq} \sum_{i=1}^{n_\ell} R_{\ell_i}^2}}{\doneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\doneover{2\sigsq} \sum_{i=1}^{n_\ell} R_{\ell_i}^2}} = 1 \eqncomment{since $n_\ell = n_{\ell_R} + n_{\ell_L}$}
%\eeqn


%\begin{changemargin}{-0.5in}{0in}
%\beqn
%\frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} &=& \frac{\oneoversqrt{\sigsq_\mu \parens{\frac{n_{\ell_L}}{\sigsq} + 1}} \oneoversqrt{\sigsq_\mu \parens{\frac{n_{\ell_R}}{\sigsq} + 1}} ~~\exp{\dfrac{n_{\ell_L}^2 \Rbar_{\ell_L}^2 \sigsq_\mu}{2\parens{\sigma^4 + n_{\ell_L}\sigsq \sigsq_\mu}}} ~~\exp{\dfrac{n_{\ell_R}^2 \Rbar_{\ell_R}^2 \sigsq_\mu}{2\parens{\sigma^4 + n_{\ell_R}\sigsq \sigsq_\mu}}}}{\oneoversqrt{\sigsq_\mu \parens{\frac{n_\ell}{\sigsq} + 1}} ~~\exp{\dfrac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\parens{\sigma^4 + n_\ell\sigsq \sigsq_\mu}}}} \\
%&=& \sqrt{\frac{\sigsq \parens{n_\ell + \sigsq}}{\sigsq_\mu \parens{n_{\ell_L} + \sigsq}\parens{n_{\ell_R} + \sigsq}}} ~~\exp{\frac{\sigsq_\mu}{2\sigsq} \parens{\frac{\overbrace{n_{\ell_L}^2 \Rbar_{\ell_L}^2}}{\sigsq + n_{\ell_L}\sigsq_\mu} + \frac{\overbrace{n_{\ell_R}^2 \Rbar_{\ell_R}^2}}{\sigsq + n_{\ell_R}\sigsq_\mu} - \frac{\overbrace{n_\ell^2 \Rbar_\ell^2}}{\sigsq + n_\ell \sigsq_\mu}}} \\
%\eeqn
%\end{changemargin}

Note how the ratio is only a function of the $\sum R_i$'s which makes calculations efficient.

%Let's think about what's really driving this ratio. Consider $\sigsq = \sigsqmu = 1$ and $n_\ell = 10$ with an equal split in left and right daughter leaves. That would mean that the ratio would be:
%
%\beqn
%\sqrt{\frac{11}{36}}~~\exp{\half \parens{\frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell_L, i}}}{6} + \frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell_R, i}}}{6} - \frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell, i}}}{11}}} \\
%\eeqn


\subsubsection*{Tree Structure Ratio}

We consider each tree separately.

\beqn
\prob{T^*} &=& \prod_{\eta \in H} \prob{\text{splitting~} \eta} \prod_{\eta \in H} \prob{\text{assigning a rule to~} \eta} \\
\eeqn

Remember from CGM98 that the probability of splitting on a given node $\eta$ is driven by two hyperparameters, $\alpha$ and $\beta$ in the following way where $d_\eta$ is the depth of the $\eta$ node:

\beqn
\prob{\text{splitting~} \eta} = \frac{\alpha}{\tothepow{1 + d_\eta}{\beta}}
\eeqn

The probability of assigning the specific rule is just picking from all available attributes and then from all available split points so $\oneover{p_\eta}\oneover{n_\eta}$. Once again, the proposal tree differs from the original tree by only the $\ell$th node in the original becoming the $\ell_L$ and $\ell_R$ nodes in the proposal. This simplifies the tree structure ratio to just:

\beqn
\frac{\prob{T^*}}{\prob{T}} = \frac{\dfrac{\alpha}{\tothepow{1 + d_{\eta_L}}{\beta}} \dfrac{\alpha}{\tothepow{1 + d_{\eta_R}}{\beta}}}{ \dfrac{\alpha}{\tothepow{1 + d_{\eta}}{\beta}}} \frac{p_\eta n_\eta}{p_{\eta_L} p_{\eta_R} n_{\eta_L} n_{\eta_R}}
\eeqn

We know the depth of the child nodes is just the depth of the parent node incremented by 1. That plus a bit more algebra gives us:

\beqn
\frac{\prob{T^*}}{\prob{T}} = \alpha \frac{\tothepow{1 + d_\eta}{\beta}}{\tothepow{2 + d_\eta}{2\beta}} \frac{p_\eta n_\eta}{p_{\eta_L} p_{\eta_R} n_{\eta_L} n_{\eta_R}}
\eeqn

Now we have a way of calculating $r$ for grow proposals by multiplying all three above results.\\

\subsection*{Prune Proposal}

\subsubsection*{Transition Ratio}

For prune proposals, we move in the opposite direction.  We need to hack off a node:

\beqn
\prob{T \rightarrow T^*} &=& \prob{\text{PRUNE}} \prob{\text{selecting the $\ell$th node to prune from}} \\
&=& \prob{\text{PRUNE}}\oneover{w_2}
\eeqn

To go the opposite direction, we need to make sure we grow the exact same node so $p_{adj}, n_{adj}$ need to be calculated based on whatever the $\ell$th node originally was:

\beqn
\prob{T^* \rightarrow T} &=& \prob{\text{GROW}} \prob{\text{selecting the $\ell$th node to grow from}} \times \\
&& \prob{\text{selecting the original attribute to split on}} \times \\
&& \prob{\text{selecting the original value to split on}} \\
&=& \prob{\text{GROW}} \oneover{b-1} \oneover{p_{adj}} \oneover{n_{adj}}
\eeqn

We're using $b-1$ here because the proposed tree has one less terminal node due to the pruning than the original tree $T$ had.\\

Thus, the transition ratio becomes:

\beqn
\frac{\prob{T^* \rightarrow T}}{\prob{T \rightarrow T^*}} = \frac{\prob{\text{GROW}}  \oneover{b-1}\oneover{p_{adj}} \oneover{n_{adj}}}{ \prob{\text{PRUNE}}\oneover{w_2}} = \frac{\prob{\text{GROW}}}{\prob{\text{PRUNE}}}\frac{w_2}{(b-1) p_{adj} n_{adj}}
\eeqn

Once again, we need to bookkeep and make sure we can actually prune this node. If it's just a root node, then we can't even consider this step at all. Otherwise, they'll cancel.

\subsubsection*{Proportional Likelihood Ratio}

It is pretty obvious this is the inverse of the grow step's proportional likelihood ratio. Now the tree proposal has just one collapsed node where the original has a left and right component:

\begin{changemargin}{-0.5in}{0in}
\beqn
\frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} &=& \inverse{\sqrt{\frac{\sigsq \parens{n_\ell + \sigsq}}{\sigsq_\mu \parens{n_{\ell_L} + \sigsq}\parens{n_{\ell_R} + \sigsq}}} ~~\exp{\frac{\sigsq_\mu}{2\sigsq} \parens{\frac{n_{\ell_L}^2 \Rbar_L^2}{\sigsq + n_{\ell_L}\sigsq_\mu} + \frac{n_{\ell_R}^2 \Rbar_R^2}{\sigsq + n_{\ell_R}\sigsq_\mu} - \frac{n_\ell^2 \Rbar^2}{\sigsq + n_\ell \sigsq_\mu}}}} \\
&=& \sqrt{\frac{\sigsq_\mu \parens{n_{\ell_L} + \sigsq}\parens{n_{\ell_R} + \sigsq}}{\sigsq \parens{n_\ell + \sigsq}}} ~~\exp{\frac{\sigsq_\mu}{2\sigsq} \parens{\frac{n_\ell^2 \Rbar^2}{\sigsq + n_\ell \sigsq_\mu} - \frac{n_{\ell_L}^2 \Rbar_L^2}{\sigsq + n_{\ell_L}\sigsq_\mu} - \frac{n_{\ell_R}^2 \Rbar_R^2}{\sigsq + n_{\ell_R}\sigsq_\mu}}} \\
\eeqn
\end{changemargin}

\subsubsection*{Tree Structure Ratio}

It is also clear this is just the inverse of the tree structure ratio for the grow step.

\beqn
\frac{\prob{T^*}}{\prob{T}} &=& \inverse{\alpha \frac{\tothepow{1 + d_\eta}{\beta}}{\tothepow{2 + d_\eta}{2\beta}} \frac{p_\eta n_\eta}{p_{\eta_L} p_{\eta_R} n_{\eta_L} n_{\eta_R}}}\\
&=& \oneover{\alpha} \frac{\tothepow{2 + d_\eta}{2\beta}}{\tothepow{1 + d_\eta}{\beta}} \frac{p_{\eta_L} p_{\eta_R} n_{\eta_L} n_{\eta_R}}{p_\eta n_\eta}
\eeqn


\subsection*{Change and Swap Proposals}

Due to the complexity of the bookkeeping, we do not consider these steps.

\subsection*{Implementation Details}

We use what we have above to calculate $r$ for grow and prune steps. In practice, we just take the log to avoid numerical problems. \\

One thing we would like to estimate is the likelihood of the trees over the lifetime of the Gibbs sampler.


\end{document}





































%The change proposal is quite different from the grow and prune. This is taking an internal node (any of the $v$ internal nodes where the root itself is included) and changing the rule by switching both the split attribute and the split rule value.\\
%
%Consider $\prob{T \rightarrow T^*} = \cprob{T^*}{T}$ during a change:
%
%\beqn
%\cprob{T^*}{T} &=& \prob{\text{CHANGE}} \prob{\text{selecting the $\ell$th node to change}} \times \\
%&& \prob{\text{selecting the $j$th attribute to split on}} \times \\
%&& \prob{\text{selecting the $i$th value to split on}} \\
%\eeqn
%
%Then going backwards have the same thing but we need to get back to the original node:
%
%\beqn
%\cprob{T}{T^*} &=& \prob{\text{CHANGE}} \prob{\text{selecting the $\ell$th node to change}} \times \\
%&& \prob{\text{selecting the original attribute to split on}} \times \\
%&& \prob{\text{selecting the original value to split on}} \\
%\eeqn
%
%The transition ratio will have many cancellations. The probability of picking a change proposal is obviously the same. The probability of picking the $\ell$th node is the same. Since there are only a certain number of attributes available to the $\ell$th node, that will be the same. This leaves us with:
%
%\beqn
%\frac{\cprob{T}{T^*}}{\cprob{T^*}{T}} = \frac{n_{\text{original}}}{n_{\text{proposal}}}
%\eeqn
%
%where $n_{\text{original}}$ is the number of split points available if the $\ell$th node's split attribute is the original attribute and $n_{\text{proposal}}$ is the number of split points available if the proposal split attribute is used instead.
%
%We now move on to the proportional likelihood ratio. We note that if we make a change, all daughter nodes are affected all the way down the lineage. Hence, we need to compare the likelihoods of all the daughter nodes underneath the $\ell$th node. There is no clean way to denote this, so I just spell it out:
%
%\beqn
%\frac{\cprob{R}{T^*, \sigsq}}{\cprob{R}{T, \sigsq}} = \frac{\prod_{\ell^* \in \text{all leaf nodes under the $\ell$th node}} { \abss{\bSigma_{\ell^*}}^{-\half} \exp{-\half R_{\ell^*}^\top \bSigmainv_{\ell^*} R_{\ell^*}}}}{\prod_{\ell \in \text{all leaf nodes under the $\ell$th node}} { \abss{\bSigma_\ell}^{-\half} \exp{-\half R_\ell^\top \bSigmainv_\ell R_\ell}}}
%\eeqn
%
%I leave out the $2\pi^{n_\cdot}$ terms because the total number $n$ at any given node will have to remain the same regardless as how it's apportioned in the nodes below it. \\
%
%\textbf{Question}: how is $R_\ell$ defined if the node is empty? \\
%
%Now we move on to the tree structure ratio. Recall that:
%
%\beqn
%\prob{T^*} &=& \prod_{\eta \in H} \prob{\text{splitting~} \eta} \prod_{\eta \in H} \prob{\text{assigning a rule to~} \eta} \\
%\eeqn
%
%Since both the proposal and original trees have the same structure, we can ignore the $\prob{\text{splitting~} \eta}$ terms when we form the ratio. And once again, only the nodes below the changed node need to be considered since all other nodes are the same for both the proposal and original. Thus we're left with:
%
%\beqn
%\frac{\prob{T^*}}{\prob{T}} = \frac{\prod_{h \in \text{all nodes under $\eta$}} \prob{\text{assigning a rule to~} h ~|~ T^*}}{\prod_{h \in \text{all nodes under $\eta$}} \prob{\text{assigning a rule to~} h ~|~ T}} \equalsquestion \frac{\prob{\text{assigning a rule to~}  \eta ~|~ T^*}}{\prob{\text{assigning a rule to~} \eta ~|~ T}}
%\eeqn
%
%The question is does is the $\equalsquestion$ above a true equality? I doubt it. Let's say your original split is $x_{37} < 89.2$ and the proposed change split is $x_{49} < 23.8$. It's possible that other $x_{49}$ rules are present underneath node $\eta$. These rule assignment probabilities will now change because the number of available split values will change. \\
%
%\textbf{Question}: what happens if the proposed change is a minimum value on an attribute, and there are nodes below with the same split attribute, that will force the numerator to be zero and the step should always be rejected? \\
%
%It seems that change steps are quite complicated when there is all this bookkeeping. This may be the reason Rob's new BART implementation only does GROWs and PRUNEs. \\
%
%I really would like to nail down this spec before I go ahead and implement these features, so if both of you could comment, that would be fantastic.
