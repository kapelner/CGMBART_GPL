\include{preamble}

\title{Some notes on the Metropolis-Hastings Implementation}

\date{}

\begin{document}
\maketitle

\section*{MCMC for BART}

According to Gelman p.291, the Metropolis-Hastings algorithm differs from the Metropolis algorithm because you need to consider the ratio of ratios:

\beqn
r = \frac{\dfrac{\cprob{\theta^*}{Y}}{J_t(\theta^* ~|~ \theta^{t-1})}}{\dfrac{\cprob{\theta^{t-1}}{Y}}{J_t(\theta^{t-1} ~|~ \theta^*)}} = \frac{J_t(\theta^{t-1} ~|~ \theta^*)}{J_t(\theta^* ~|~ \theta^{t-1})}\frac{\prob{\theta^* ~|~ Y}}{\prob{\theta^{t-1} ~|~ Y}}
\eeqn

Where we accept if a random draw from a uniform is less than the ratio above \ie $X \sim \uniform{0}{1} \leadsto x < r$. \\

The parameters of interest in our case are the new tree (which is created from one of three types of proposals), which we denote $T^*$, and the original tree, denoted by $T$. For jump notation which is $J$, we denote this just using the regular probability symbol (to be boring). We denote the residual left over from $Y$, the vector of random variables, as $\R := \bracks{\Roneton}^\top$. We denote $\sigsq$ as the random variable estimating the homoskedastic noise in the model which is sampled after all the trees are sampled. We do not notate all the hyperparameters to avoid notational messiness. The M-H ratio then becomes:

\beqn
r = \frac{\prob{T^* \rightarrow T}}{\prob{T \rightarrow T^*}} \frac{\cprob{T^*}{\R, \sigsq}}{\cprob{T}{\R, \sigsq}}
\eeqn

%We then use Bayes Rule to obtain the following which I partition form. We partition the ratio into three interpretable terms:

%\beqn
%r = \underbrace{\frac{\cprob{T}{T^*}}{\cprob{T^*}{T}}}_{\text{transitioning ratio}} \times \overbrace{\frac{\cprob{R}{T^*, \sigsq}}{\cprob{R}{T, \sigsq}}}^{\text{likelihood ratio}} \times \underbrace{\frac{\prob{T^*}}{\prob{T}}}_{\text{tree structure ratio}}
%\eeqn

My goal is to come up with an exact way of calculating $r$ for all possible tree proposals.\\

Throughout this document, we use the following notation:

\begin{itemize}
\item $\Hint$ ---  the collection of internal nodes in tree $T$
\item $\Hleaves$ ---  the collection of leaves in tree $T$
\item $\eta$ --- a node $\in \Hint \cup \Hleaves$.
\item $d_\eta$ --- the depth of the $\eta$th node. The root node is defined as having depth 0, its first child has depth 1, etc.
\item $b$ --- the number of terminal nodes / leaves in the tree $T$, $\abss{\Hleaves}$. These are the nodes that can potentially be ``grown.'' For example, the number of terminal nodes in the $T^*$ tree is $b+1$ if $T$ was grown and $b-1$ if $T$ was pruned.
\item $w_2$ --- the number of 2nd generation internal nodes in tree $T$, $\abss{\Hint}$ \ie the number of nodes who only have two children. These are the nodes that can potentially be ``pruned.''
\item $w_2^*$ --- the number of 2nd generation internal nodes in tree $T^*$, $\abss{\Hint^*}$. This can be equal to $w$ or $w+1$ so it has to be recalculated. Draw a few pictures of trees and grow steps and you'll see why this unfortunate fact is so.
%\item $w$ --- the number of internal nodes regardless of generation (even the root is included in this count).
\item  $\ell$ --- the index of the terminal node which we've ``picked'' to grow from it is a number $\in \braces{1, \ldots, b}$.
\item $\ell_L$ and $\ell_R$ --- represent the new left and right node indices in tree $T^*$ which are ``grown'' from the $\ell$th node of tree $T$
\end{itemize}

\subsection*{Likelihood Calculation}

It is imperative that we can calculate $\cprob{T}{\R, \sigsq}$ for the calculation of $r$. Let's analyze carefully what it means to get the likelihood of a tree.

First, note that the likelihood for $T$ given the data is not defined in our model, so we use Bayes Rule to obtain something that is tractable in our model:

\beqn
\cprob{T}{\R, \sigsq} = \frac{\cprob{\R}{T, \sigsq} \cprob{T}{\sigsq}}{\cprob{\R}{\sigsq}}
\eeqn

What's in the numerator? The likelihood of the data given the tree and the variance times the probability of the tree given the variance. The probability of the tree is based on probabilities of splits and rules and is not dependent on the variance, $\cprob{T}{\sigsq} = \prob{T}$, so we can already simplify to:

\beqn
\cprob{T}{\R, \sigsq} = \frac{\cprob{\R}{T, \sigsq} \prob{T}}{\cprob{\R}{\sigsq}}
\eeqn

What about the denominator --- the probability of the data? The probability of the data is weighted over every possible tree configuration:

\beqn
\cprob{\R}{\sigsq} = \int_{T \in \mathcal{T}} \cprob{\R}{T, \sigsq} \prob{T} dT
\eeqn

and removing the dependency on $T$ becomes:

\beqn
\cprob{\R}{\sigsq} = \int_{T \in \mathcal{T}} \prod_{\ell=1}^{b_T} \cprob{\Rlonetonl}{\sigsq} \prob{T} dT
\eeqn

which of course is arrived at via the margining out of the means of each leaf:

\beqn
\cprob{\R}{\sigsq} = \int_{T \in \mathcal{T}} \parens{\prod_{\ell=1}^{b_T} \int_\reals \cprob{\Rlonetonl}{\mu, \sigsq} \prob{\mu; \sigsq_\mu} d\mu} \prob{T} dT
\eeqn

The point being is that this quantity is the same for all data $\R$. This is useful since we're creating ratios where we're using the same data, and this quantity will cancel.

Now let's look at the ratio of the likelihoods which is what we care about for the calculation of $r$. I identify three pieces which we will use for the next couple of sections:

\beqn
r &=& \frac{\prob{T^* \rightarrow T}}{\prob{T \rightarrow T^*}} \underbrace{\frac{\cprob{T^*}{\R, \sigsq}}{\cprob{T}{\R, \sigsq}}} \\
&& \quad\quad\quad\quad~~ \frac{\dfrac{\cprob{\R}{T^*, \sigsq} \prob{T^*}}{\cancel{\cprob{\R}{\sigsq}}}}{\dfrac{\cprob{\R}{T, \sigsq} \prob{T}}{\cancel{\cprob{\R}{\sigsq}}}} \\
&=& \underbrace{\frac{\prob{T^* \rightarrow T}}{\prob{T \rightarrow T^*}}}_{\text{transition ratio}} ~~~\times~~~ \underbrace{\frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}}}_{\text{likelihood ratio}} ~~~\times \underbrace{\frac{\prob{T^*}}{\prob{T}}}_{\text{tree structure ratio}}
\eeqn

\subsection*{Grow Proposal}

Let's pretend we're transitioning from $T \rightarrow T^*$ using a GROW step and let's analyze each of the above expressions one-by-one.

\subsubsection*{Transition Ratio}

So $\prob{T \rightarrow T^*}$ means the probability of transitioning from $T$ into the new tree proposal $T^*$. This would have to be equal to the following:

\beqn
\prob{T \rightarrow T^*} &=& \prob{\text{GROW}} \prob{\text{selecting the $\ell$th node to grow from}} \times \\
&& \prob{\text{selecting the $j$th attribute to split on}} \prob{\text{selecting the $i$th value to split on}}
\eeqn

We're picking from one of the terminal nodes, and then we're picking an attribute and split point, this becomes:

\beqn
\cprob{T^*}{T} &=& \prob{\text{GROW}} \oneover{b} \oneover{p_{adj}} \oneover{n_{adj}}
\eeqn

Now, $p_{adj}$ is the number of predictors left available to split on. This is \textit{from the perspective} of the $\ell$th node in tree $T$. Why would this be less than $p$? Because if you look up into the node's lineage, you may have already used all available split values for some attributes. Those would no longer be available to split from.

Then, $n_{adj}$ is the number of split values left available considering we picked attribute $j$. We can obtain this by looking at the node's lineage for any splits on $j$ and then taking the minimum of those split values, and then finding the subset of the design matrix whose values are less than that minimum for column $j$.\\

So now $\prob{T^* \rightarrow T}$ is the probability of transitioning from the new tree back to the old tree which would be:

\beqn
\prob{T^* \rightarrow T} &=& \prob{\text{PRUNE}} \prob{\text{selecting the $\ell$th node to prune from}} \\
&=& \prob{\text{PRUNE}}\oneover{w_2^*}
\eeqn

Thus, the transition ratio will be:

\beqn
\frac{\prob{T^* \rightarrow T}}{\prob{T \rightarrow T^*}} = \frac{\prob{\text{PRUNE}}\oneover{w_2^*}}{\prob{\text{GROW}}\oneover{b} \oneover{p_{adj}} \oneover{n_{adj}}} = \cancelto{?}{\frac{\prob{\text{PRUNE}}}{\prob{\text{GROW}}}} \frac{b ~ p_{adj} ~ n_{adj}}{w_2^*}
\eeqn

Why don't the probabilities of prune and grow cancel? Well, under the case where we cannot grow anymore (if we use all split variables), the probability of growth will be 0. Thus, those steps \textit{cannot} be considered since the ratio would be undefined. As long as they can be considered, that ratio will cancel.

\subsubsection*{Likelihood Ratio}


What about the likelihood given the tree? The only reason you need the $T$ information is so we know which $R$ values fall in which of the leaves:

\beqn
\cprob{\Roneton}{T, \sigsq} = \prod_{\ell=1}^{b} \underbrace{\cprob{\Rlonetonl}{\sigsq}}
\eeqn

The r.h.s underbraces is the likelihood of each leaf separately which is \textit{not} dependent on $T$ anymore. We can multiply the likelihoods of each leaf because we assume the leaves are independent. The $R_\ell$'s are the data in the $\ell$th leaf and there is $n_\ell$ of them, the portion of $n$ in the leaf. Obviously, $n = \sum_{\ell=1}^b n_\ell$.\\

Let's look at the likelihood of a single leaf more carefully. We know that if we knew the mean at the leaf, which we denote $\mu_\ell$, we would have:

\beqn
\Rlonetonl | \mu_\ell, \sigsq ~\iid~ \normnot{\mu}{\sigsq}
\eeqn

This means that if we can margin out $\mu$, we can arrive at the expression that is needed for the calculation. Recall that one of the BART model assumptions is a prior on the average value of $\mu \sim \normnot{0}{\sigsq_\mu}$ and thus:

\beqn
\cprob{\Rlonetonl}{\sigsq} = \int_\reals \cprob{\Rlonetonl}{\mu, \sigsq} \prob{\mu; \sigsq_\mu} d\mu
\eeqn

Since the likelihoods are solely determined by the terminal nodes, the proposal tree differs from the original tree by only the $\ell$th node in the original becoming the $\ell_L$ and $\ell_R$ nodes in the proposal. Hence, the likelihood ratio becomes only:

\beqn
&& \frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} = \frac{\cprob{\RLlonetonlL}{\sigsq} \cprob{\RRlonetonlR}{\sigsq}}{\cprob{\Rlonetonl}{\sigsq}} \\
&=& \frac{\myint{\mu}{\reals}{}{\cprob{\RLlonetonlL}{\mu, \sigsq}\prob{\mu}} ~~ \myint{\mu}{\reals}{}{\cprob{\RRlonetonlR}{\mu, \sigsq}\prob{\mu}}}{\myint{\mu}{\reals}{}{\cprob{\Rlonetonl}{\mu, \sigsq}\prob{\mu}}}  \eqncomment{1}\\
\eeqn

I now present three strategies to perform the margining.

\subsubsection*{Strategy \#1 --- Convolution}

Let's take a more careful look at the non-margined leaf-likelihood expression and recall from basic mathematical statistics the result that the sample average, $\Rbar_\ell$, is a sufficient statistic for $\mu$ via the factorization theorem:


\begin{changemargin}{-0.5in}{0in}
\beqn
\prob{\Rlonetonl | \mu, \sigsq} &=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \mu}} \\
&=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \parens{n_\ell \squared{\Rbar_\ell - \mu} + \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}}} \\
&=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}} ~\exp{-\frac{n_\ell}{2\sigsq} \squared{\Rbar_\ell - \mu}} \\
&=& \underbrace{\oneoversqrt{n_\ell} \oneover{\tothepow{2\pi\sigsq}{\overtwo{n_\ell - 1}}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}}}_h ~ \sqrt{\frac{n_\ell}{2\pi\sigsq}} ~\exp{-\frac{n_\ell}{2\sigsq} \squared{\Rbar_\ell - \mu}} \\
\eeqn
\end{changemargin}


So now we margin by integrating:

\begin{changemargin}{-0.5in}{0in}
\beqn
\cprob{\Rlonetonl}{\sigsq} &=& \int_\reals \prob{\Rlonetonl | \mu_\ell, \sigsq} \prob{\mu_\ell; \sigsq_\mu} d\mu_\ell \\
&=& h \underbrace{\int_\reals \sqrt{\frac{n_\ell}{2\pi\sigsq}} ~\exp{-\frac{n_\ell}{2\sigsq} \squared{\Rbar_\ell - \mu_\ell}} \oneoversqrt{2\pi\sigsq_\mu} ~\exp{-\oneover{2\sigsq_\mu} \musq}  d\mu_\ell}_{\text{the definition of a convolution}} \\
&=& h ~\normnot{0}{\frac{\sigsq}{n_\ell}} \star \normnot{0}{\sigsq_\mu} \\
&=& h ~ \normnot{0}{\frac{\sigsq}{n_\ell} + \sigsq_\mu} \eqncomment{with free variable $\Rbar_\ell$} \\
&=& h ~ \oneoversqrt{2\pi \parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}} ~\exp{-\oneover{2 \parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}} \Rbar_\ell^2}\\
\eeqn
\end{changemargin}

%The problem here is that $g$ is not a likelihood. So I don't know how to do that margining trick with $\normnot{0}{\overn{\sigsq}} \star \normnot{0}{\sigsq_\mu} = \normnot{0}{\overn{\sigsq} + \sigsq_\mu}$. So I think I have to do the integral manually using Mathematica:
%
%\beqn
%\int_\reals g(\Rbar_\ell ~|~ \mu, \sigsq) \prob{\mu; \sigsq_\mu} d\mu &=& \int_\reals \exp{-\oneover{2\sigsq} \parens{-2n_\ell \mu \Rbar_\ell + n_\ell \musq}} \oneoversqrt{2\pi\sigsq_\mu} \exp{-\oneover{2\sigsq_\mu} \musq} d\mu \\
%&=& \oneoversqrt{\sigsq_\mu \parens{\frac{n_\ell}{\sigsq} + 1}} \exp{\frac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\sigsq\parens{\sigsq + n_\ell \sigsq_\mu}}} \\
%\Rightarrow \cprob{\Rlonetonl}{\sigsq} &\propto& \oneoversqrt{\sigsq_\mu \parens{\frac{n_\ell}{\sigsq} + 1}} \exp{\frac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\sigsq\parens{\sigsq + n_\ell \sigsq_\mu}}} \\
%\eeqn

%\begin{figure}
%\begin{center}
%\includegraphics[width=6in]{integrate_out_mu2.eps}
%\end{center}
%\end{figure}
%\FloatBarrier

%And this result does not appear to be equal to $\normnot{0}{\overn{\sigsq} + \sigsq_\mu}$. \\


%\beqn
%\cprob{\Rlonetonl}{\sigsq} = h(\Rlonetonl ~|~ \sigsq) \oneoversqrt{\sigsq_\mu \parens{\frac{n_\ell}{\sigsq} + 1}} \exp{\frac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\parens{\sigma^4 + n_\ell\sigsq \sigsq_\mu}}}
%\eeqn

So now we substitue the above result into equation 1:

\begin{changemargin}{-0.5in}{0in}
\beqn
&& \frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} \\
&=& \frac{h_L ~ \oneoversqrt{2\pi \parens{\frac{\sigsq}{n_{\ell_L}} + \sigsq_\mu}} ~\exp{-\oneover{2 \parens{\frac{\sigsq}{n_{\ell_L}} + \sigsq_\mu}} \Rbar_{\ell_L}^2} h_R ~ \oneoversqrt{2\pi \parens{\frac{\sigsq}{n_{\ell_R}} + \sigsq_\mu}} ~\exp{-\oneover{2 \parens{\frac{\sigsq}{n_{\ell_R}} + \sigsq_\mu}} \Rbar_{\ell_R}^2}}{h ~ \oneoversqrt{2\pi \parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}} ~\exp{-\oneover{2 \parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}} \Rbar_\ell^2}} \\
&=& \frac{h_L h_R}{h}  \sqrt{\frac{\cancel{2\pi} \parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}}{\cancel{2\pi} \parens{\frac{\sigsq}{n_{\ell_L}} + \sigsq_\mu} 2\pi \parens{\frac{\sigsq}{n_{\ell_R}} + \sigsq_\mu}}} \frac{~\exp{-\oneover{2 \parens{\frac{\sigsq}{n_{\ell_L}} + \sigsq_\mu}} \Rbar_{\ell_L}^2}  ~\exp{-\oneover{2 \parens{\frac{\sigsq}{n_{\ell_R}} + \sigsq_\mu}} \Rbar_{\ell_R}^2}}{\exp{-\oneover{2 \parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}} \Rbar_\ell^2}} \\
&=& \frac{h_L h_R}{h}  \sqrt{\frac{ \parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}}{ \parens{\frac{\sigsq}{n_{\ell_L}} + \sigsq_\mu} 2\pi \parens{\frac{\sigsq}{n_{\ell_R}} + \sigsq_\mu}}} \exp{\oneover{2 \parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}} \Rbar_\ell^2 -\oneover{2 \parens{\frac{\sigsq}{n_{\ell_L}} + \sigsq_\mu}} \Rbar_{\ell_L}^2 -\oneover{2 \parens{\frac{\sigsq}{n_{\ell_R}} + \sigsq_\mu}} \Rbar_{\ell_R}^2} \\
\eeqn
\end{changemargin}

%\beqn
%\frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} &=& h\parens{\RLlonetonlL ~|~ \sigsq} \oneoversqrt{\sigsq_\mu \parens{\frac{n_{\ell_L}}{\sigsq} + 1}} ~~\exp{\frac{n_{\ell_L}^2 \Rbar_{\ell_L}^2 \sigsq_\mu}{2\parens{\sigma^4 + n_{\ell_L}\sigsq \sigsq_\mu}}} \\
%&& ~~\times h\parens{\RRlonetonlR ~|~ \sigsq} \oneoversqrt{\sigsq_\mu \parens{\frac{n_{\ell_R}}{\sigsq} + 1}} ~~\exp{\frac{n_{\ell_R}^2 \Rbar_{\ell_R}^2 \sigsq_\mu}{2\parens{\sigma^4 + n_{\ell_R}\sigsq \sigsq_\mu}}} \\
%&& ~~\times \inverse{h\parens{\Rlonetonl ~|~ \sigsq} \oneoversqrt{\sigsq_\mu \parens{\frac{n_\ell}{\sigsq} + 1}} ~~\exp{\dfrac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\parens{\sigma^4 + n_\ell\sigsq \sigsq_\mu}}}} \\
%\eeqn

Let's examine the ratio of the $h$ functions:

\begin{changemargin}{-1in}{0in}
\beqn
&& \frac{h_L h_R}{h} = \frac{\doneoversqrt{n_{\ell_L}} \doneover{\tothepow{2\pi\sigsq}{\overtwo{n_{\ell_L} - 1}}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_{\ell_L}} \squared{R_{\ell_L, i} - \Rbar_{\ell_L}}} \doneoversqrt{n_{\ell_R}} \doneover{\tothepow{2\pi\sigsq}{\overtwo{n_{\ell_R} - 1}}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_{\ell_R}} \squared{R_{\ell_R, i} - \Rbar_{\ell,R}}}}{\doneoversqrt{n_\ell} \doneover{\tothepow{2\pi\sigsq}{\overtwo{n_\ell - 1}}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}}} \\
\eeqn
\end{changemargin}

Now note that:

\beqn
\frac{\doneover{\tothepow{2\pi\sigsq}{\overtwo{n_{\ell_L} - 1}}} \doneover{\tothepow{2\pi\sigsq}{\overtwo{n_{\ell_R} - 1}}}}{\doneover{\tothepow{2\pi\sigsq}{\overtwo{n_\ell - 1}}}} = \frac{\tothepow{2\pi\sigsq}{\overtwo{n_\ell - 1}}}{\tothepow{2\pi\sigsq}{\overtwo{n_\ell - 2}}} = \sqrt{2\pi\sigsq}
\eeqn

Grouping the rest of the terms inside and outside of the exponentiation:

\beqn
\frac{h_L h_R}{h} &=& \sqrt{\frac{2\pi\sigsq n_\ell}{n_{\ell_L}n_{\ell_R}}} ~\exp{-\oneover{2\sigsq}\parens{\underbrace{\sum_{i=1}^{n_{\ell_L}} \squared{R_{\ell_L, i} - \Rbar_{\ell_L}} + \sum_{i=1}^{n_{\ell_R}} \squared{R_{\ell_R, i} - \Rbar_{\ell,R}} - \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}}}}
\eeqn

Now let's take a look at the underbraced quantity:

\beqn
&=& \sum_{i=1}^{n_{\ell_L}} R_{\ell_L, i}^2 - 2n_{\ell_L}\Rbar_{\ell_L}^2 + n_{\ell_L}\Rbar_{\ell_L}^2 +\sum_{i=1}^{n_{\ell_R}} R_{\ell_R, i}^2 - 2n_{\ell_R}\Rbar_{\ell_R}^2 + n_{\ell_R}\Rbar_{\ell_R}^2 - \sum_{i=1}^{n_{\ell}} R_{\ell, i}^2 + 2n_{\ell}\Rbar_{\ell}^2 - n_{\ell}\Rbar_{\ell}^2 \\
&=& \cancel{\sum_{i=1}^{n_{\ell_L}} R_{\ell_L, i}^2} - 2n_{\ell_L}\Rbar_{\ell_L}^2 + n_{\ell_L}\Rbar_{\ell_L}^2 + \cancel{\sum_{i=1}^{n_{\ell_R}} R_{\ell_R, i}^2} - 2n_{\ell_R}\Rbar_{\ell_R}^2 + n_{\ell_R}\Rbar_{\ell_R}^2 - \cancel{\sum_{i=1}^{n_{\ell}} R_{\ell, i}^2} + 2n_{\ell}\Rbar_{\ell}^2 - n_{\ell}\Rbar_{\ell}^2 \\
&=& n_{\ell}\Rbar_{\ell}^2 - n_{\ell_L}\Rbar_{\ell_L}^2 - n_{\ell_R}\Rbar_{\ell_R}^2
\eeqn

Thus, we can arrive at the $h$ ratio's final expression:

\beqn
\frac{h_L h_R}{h} &=& \sqrt{\frac{2\pi\sigsq n_\ell}{n_{\ell_L}n_{\ell_R}}} ~\exp{-\oneover{2\sigsq} \parens{n_{\ell}\Rbar_{\ell}^2 - n_{\ell_L}\Rbar_{\ell_L}^2 - n_{\ell_R}\Rbar_{\ell_R}^2}} \\
\eeqn

Now, putting it all together:

\begin{changemargin}{-0.5in}{0in}
\beqn
&& \frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} \\
&=& \sqrt{\frac{\cancel{2\pi} \sigsq n_\ell}{n_{\ell_L}n_{\ell_R}}} \sqrt{\frac{ \parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}}{ \parens{\frac{\sigsq}{n_{\ell_L}} + \sigsq_\mu} \cancel{2\pi} \parens{\frac{\sigsq}{n_{\ell_R}} + \sigsq_\mu}}} ~\exp{-\oneover{2\sigsq} \parens{n_{\ell}\Rbar_{\ell}^2 - n_{\ell_L}\Rbar_{\ell_L}^2 - n_{\ell_R}\Rbar_{\ell_R}^2}} \times \\
&& \exp{\half \parens{\oneover{\parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}} \Rbar_\ell^2 -\oneover{ \parens{\frac{\sigsq}{n_{\ell_L}} + \sigsq_\mu}} \Rbar_{\ell_L}^2 -\oneover{\parens{\frac{\sigsq}{n_{\ell_R}} + \sigsq_\mu}} \Rbar_{\ell_R}^2}} \\
&=& \sqrt{\frac{\sigsq n_\ell}{n_{\ell_L}n_{\ell_R}} \frac{ \parens{\frac{\sigsq}{n_\ell} + \sigsq_\mu}}{ \parens{\frac{\sigsq}{n_{\ell_L}} + \sigsq_\mu} \parens{\frac{\sigsq}{n_{\ell_R}} + \sigsq_\mu}}} ~\exp{\oneover{2\sigsq} \parens{n_{\ell_L}\Rbar_{\ell_L}^2 + n_{\ell_R}\Rbar_{\ell_R}^2 - n_{\ell}\Rbar_{\ell}^2}} \times \\
&&  \exp{\oneover{2\sigsq} \parens{\oneover{\parens{\frac{1}{n_\ell} + \frac{\sigsqmu}{\sigsq}}} \Rbar_\ell^2 -\oneover{\parens{\frac{1}{n_{\ell_L}} + \frac{\sigsqmu}{\sigsq}}} \Rbar_{\ell_L}^2 -\oneover{\parens{\frac{1}{n_{\ell_R}} + \frac{\sigsqmu}{\sigsq}}} \Rbar_{\ell_R}^2}} \\
&=& \underbrace{\sqrt{\frac{\sigsq \parens{\sigsq + \sigsq_\mu n_{\ell_L}}}{ \parens{\sigsq + \sigsq_\mu n_{\ell_L}} \parens{\sigsq + \sigsq_\mu n_{\ell_R}}}}}_c \times \\
&&  \exp{\oneover{2\sigsq} \parens{\parens{\oneover{\parens{\frac{1}{n_\ell} + \frac{\sigsqmu}{\sigsq}}} - n_\ell} \Rbar_\ell^2 -\parens{\oneover{\parens{\frac{1}{n_{\ell_L}} + \frac{\sigsqmu}{\sigsq}}} + n_{\ell_L}} \Rbar_{\ell_L}^2 - \parens{\oneover{\parens{\frac{1}{n_{\ell_R}} + \frac{\sigsqmu}{\sigsq}}} + n_{\ell_R}} \Rbar_{\ell_R}^2}} \\
&=& c ~\exp{\oneover{2\sigsq} \parens{-\parens{\frac{\frac{\sigsqmu n_{\ell}}{\sigsq}}{\parens{\frac{1}{n_\ell} + \frac{\sigsqmu}{\sigsq}}}} \Rbar_\ell^2 + \parens{\frac{\frac{\sigsqmu n_{\ell_L}}{\sigsq}}{\parens{\frac{1}{n_{\ell_L}} + \frac{\sigsqmu}{\sigsq}}}} \Rbar_{\ell_L}^2 + \parens{\frac{\frac{\sigsqmu n_{\ell_R}}{\sigsq}}{\parens{\frac{1}{n_{\ell_R}} + \frac{\sigsqmu}{\sigsq}}}} \Rbar_{\ell_R}^2}} \\
&=& c ~\exp{\oneover{2\sigsq} \parens{\parens{\frac{\sigsqmu n_{\ell_L}}{\parens{\frac{\sigsq}{n_{\ell_L}} + \sigsqmu}}} \Rbar_{\ell_L}^2 + \parens{\frac{\sigsqmu n_{\ell_R}}{\parens{\frac{\sigsq}{n_{\ell_R}} + \sigsqmu}}} \Rbar_{\ell_R}^2} - \parens{\frac{\sigsqmu n_{\ell}}{\parens{\frac{\sigsq}{n_\ell} + \sigsqmu}}} \Rbar_\ell^2} \\
&=& \sqrt{\frac{\sigsq \parens{\sigsq + \sigsq_\mu n_{\ell_L}}}{ \parens{\sigsq + \sigsq_\mu n_{\ell_L}} \parens{\sigsq + \sigsq_\mu n_{\ell_R}}}} \exp{\frac{\sigsqmu}{2\sigsq} \parens{ \frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell_L, i}}}{\parens{\sigsq + n_{\ell_L}\sigsqmu}} + \frac{\squared{\sum_{i=1}^{n_{\ell_R}} R_{\ell_R, i}}}{\parens{\sigsq + n_{\ell_R}\sigsqmu}} - \frac{\squared{\sum_{i=1}^{n_{\ell}} R_{\ell, i}}}{\parens{\sigsq + n_\ell \sigsqmu}}}} \\
\eeqn
\end{changemargin}


\subsubsection*{Strategy \#2 --- Integral Computation in Mathematica}

Let's factorize the $\iid$ normal likelihood like the following:

\beqn
\prob{\Rlonetonl | \mu, \sigsq} &=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \mu}} \\
&=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} R_{\ell_i}^2} \underbrace{\exp{-\oneover{2\sigsq} \parens{-2n_\ell \mu \Rbar_\ell + n_\ell \musq}}}_g \\
%&=& h(\Rlonetonl ~|~ \sigsq) \cdot g(\Rbar_\ell ~|~ \mu, \sigsq) \\
\eeqn

Now, let's try to build the ratio:

\small
\begin{changemargin}{-1in}{0in}
\beqn
&& \frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} \\
&=& \frac{\cprob{\RLlonetonlL}{\sigsq} \cprob{\RRlonetonlR}{\sigsq}}{\cprob{\Rlonetonl}{\sigsq}} \\
&=& \frac{\myint{\mu}{\reals}{}{\cprob{\RLlonetonlL}{\mu, \sigsq}\prob{\mu}} ~~ \myint{\mu}{\reals}{}{\cprob{\RRlonetonlR}{\mu, \sigsq}\prob{\mu}}}{\myint{\mu}{\reals}{}{\cprob{\Rlonetonl}{\mu, \sigsq}\prob{\mu}}} \\
&=& \frac{\oneover{\tothepow{2\pi\sigsq}{n_{\ell_L} / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_{\ell_L}} R_{{\ell_L}_i}^2} \parens{\myint{\mu}{\reals}{}{g_{\ell_L} \prob{\mu}}}~ \oneover{\tothepow{2\pi\sigsq}{n_{\ell_R} / 2}}  ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_{\ell_R}} R_{{\ell_R}_i}^2} \parens{\myint{\mu}{\reals}{}{g_{\ell_R} \prob{\mu}}}}{\oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} R_{\ell_i}^2} \parens{\myint{\mu}{\reals}{}{g_{\ell} \prob{\mu}}}} \\
&=& \frac{\cancel{\oneover{\tothepow{2\pi\sigsq}{n_{\ell_L} / 2}}} ~\cancel{\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_{\ell_L}} R_{{\ell_L}_i}^2}} \parens{\myint{\mu}{\reals}{}{g_{\ell_L} \prob{\mu}}}~ \cancel{\oneover{\tothepow{2\pi\sigsq}{n_{\ell_R} / 2}}}  ~\cancel{\exp{-\oneover{2\sigsq}} \sum_{i=1}^{n_{\ell_R}} R_{{\ell_R}_i}^2} \parens{\myint{\mu}{\reals}{}{g_{\ell_R} \prob{\mu}}}}{\cancel{\oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}}} ~\cancel{\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} R_{\ell_i}^2}} \parens{\myint{\mu}{\reals}{}{g_{\ell} \prob{\mu}}}} \\
&=& \frac{\myint{\mu}{\reals}{}{g_{\ell_L} \prob{\mu}} \myint{\mu}{\reals}{}{g_{\ell_R} \prob{\mu}}}{\myint{\mu}{\reals}{}{g_{\ell} \prob{\mu}}} \\
&=& \frac{\myint{\mu}{\reals}{}{g_{\ell_L} \cancel{\oneoversqrt{2\pi\sigsqmu}} \exp{-\oneover{2\sigsqmu} \musq}} \myint{\mu}{\reals}{}{g_{\ell_R} \oneoversqrt{2\pi\sigsq_\mu} \exp{-\oneover{2\sigsqmu} \musq}}}{\myint{\mu}{\reals}{}{g_{\ell} \cancel{\oneoversqrt{2\pi\sigsq_\mu}} \exp{-\oneover{2\sigsqmu} \musq}}} \\
&=& \oneoversqrt{2\pi\sigsq_\mu} \frac{\myint{\mu}{\reals}{}{\exp{-\oneover{2\sigsq} \parens{-2n_{\ell_L} \mu \Rbar_{\ell_L} + n_{\ell_L} \musq}} \exp{-\oneover{2\sigsqmu} \musq}} \myint{\mu}{\reals}{}{\exp{-\oneover{2\sigsq} \parens{-2n_{\ell_L} \mu \Rbar_{\ell_L} + n_{\ell_L} \musq}}  \exp{-\oneover{2\sigsqmu} \musq}}}{\underbrace{\myint{\mu}{\reals}{}{\exp{-\oneover{2\sigsq} \parens{-2n_\ell \mu \Rbar_\ell + n_\ell \musq}} \exp{-\oneover{2\sigsqmu} \musq}}}} \\
\eeqn
\end{changemargin}
\normalsize

Let's evaluate the underbraced integral using Mathematica:

%The problem here is that $g$ is not a likelihood. So I don't know how to do that margining trick with $\normnot{0}{\overn{\sigsq}} \star \normnot{0}{\sigsq_\mu} = \normnot{0}{\overn{\sigsq} + \sigsq_\mu}$. So I think I have to do the integral manually using Mathematica:

\beqn
\int_\reals g(\Rbar_\ell ~|~ \mu, \sigsq) \prob{\mu; \sigsq_\mu} d\mu &=& \int_\reals \exp{-\oneover{2\sigsq} \parens{-2n_\ell \mu \Rbar_\ell + n_\ell \musq}} \exp{-\oneover{2\sigsq_\mu} \musq} d\mu \\
&=& \sqrt{\frac{2\pi}{\frac{n_\ell}{\sigsq} + \oneover{\sigsqmu}}} \exp{\frac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\sigsq\parens{\sigsq + n_\ell \sigsq_\mu}}} \\
%\Rightarrow \cprob{\Rlonetonl}{\sigsq} &\propto& \oneoversqrt{\sigsq_\mu \parens{\frac{n_\ell}{\sigsq} + 1}} \exp{\frac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\sigsq\parens{\sigsq + n_\ell \sigsq_\mu}}} \\
\eeqn

So now we have:

\small
\begin{changemargin}{-1in}{0in}
\beqn
&& \frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} \\
&=& \oneoversqrt{\cancel{2\pi}\sigsq_\mu} \frac{\sqrt{\dfrac{\cancel{2\pi}}{\frac{n_{\ell_L}}{\sigsq} + \oneover{\sigsqmu}}} \exp{\dfrac{n_{\ell_L}^2 \Rbar_{\ell_L}^2 \sigsq_\mu}{2\sigsq\parens{\sigsq + n_{\ell_L} \sigsq_\mu}}} \sqrt{\dfrac{\cancel{2\pi}}{\frac{n_{\ell_R}}{\sigsq} + \oneover{\sigsqmu}}} \exp{\dfrac{n_{\ell_R}^2 \Rbar_{\ell_R}^2 \sigsq_\mu}{2\sigsq\parens{\sigsq + n_{\ell_R} \sigsq_\mu}}}}{\sqrt{\dfrac{\cancel{2\pi}}{\frac{n_\ell}{\sigsq} + \oneover{\sigsqmu}}} \exp{\dfrac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\sigsq\parens{\sigsq + n_\ell \sigsq_\mu}}}} \\
&=& \sqrt{\frac{\frac{n_\ell}{\sigsq} + \oneover{\sigsqmu}}{\sigsqmu \parens{\frac{n_{\ell_L}}{\sigsq} + \oneover{\sigsqmu}}\parens{\frac{n_{\ell_R}}{\sigsq} + \oneover{\sigsqmu}}}} ~~\exp{\frac{\sigsq_\mu}{2\sigsq} \parens{\frac{n_{\ell_L}^2 \Rbar_{\ell_L}^2}{\sigsq + n_{\ell_L}\sigsq_\mu} + \frac{n_{\ell_R}^2 \Rbar_{\ell_R}^2}{\sigsq + n_{\ell_R}\sigsq_\mu} - \frac{n_\ell^2 \Rbar_\ell^2}{\sigsq + n_\ell \sigsq_\mu}}} \\
&=& \sqrt{\frac{\sigsq \parens{\sigsq + n_\ell \sigsqmu}}{\parens{\sigsq + n_{\ell_L} \sigsqmu}\parens{\sigsq + n_{\ell_R} \sigsqmu}}}~~\exp{\frac{\sigsq_\mu}{2\sigsq} \parens{\frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell_L, i}}}{\sigsq + n_{\ell_L}\sigsq_\mu} + \frac{\squared{\sum_{i=1}^{n_{\ell_R}} R_{\ell_R, i}}}{\sigsq + n_{\ell_R}\sigsq_\mu} - \frac{\squared{\sum_{i=1}^{n_{\ell}} R_{\ell, i}}}{\sigsq + n_\ell \sigsq_\mu}}} \\
\eeqn
\end{changemargin}
\normalsize

%\begin{figure}
%\begin{center}
%\includegraphics[width=6in]{integrate_out_mu2.eps}
%\end{center}
%\end{figure}
%\FloatBarrier


%\beqn
%&& \frac{h\parens{\RLlonetonlL ~|~ \sigsq}h\parens{\RRlonetonlR ~|~ \sigsq}}{h\parens{\Rlonetonl ~|~ \sigsq}} \\
%&=& \frac{\doneover{\tothepow{2\pi\sigsq}{n_{\ell_L} / 2}} ~\exp{-\doneover{2\sigsq} \sum_{i=1}^{n_{\ell_L}} R_{\ell_i}^2} \doneover{\tothepow{2\pi\sigsq}{n_{\ell_R} / 2}} ~\exp{-\doneover{2\sigsq} \sum_{i=1}^{n_{\ell_R}} R_{\ell_i}^2}}{\doneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\doneover{2\sigsq} \sum_{i=1}^{n_\ell} R_{\ell_i}^2}} \\
%&=& \frac{\doneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\doneover{2\sigsq} \sum_{i=1}^{n_\ell} R_{\ell_i}^2}}{\doneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\doneover{2\sigsq} \sum_{i=1}^{n_\ell} R_{\ell_i}^2}} = 1 \eqncomment{since $n_\ell = n_{\ell_R} + n_{\ell_L}$}
%\eeqn


%\begin{changemargin}{-0.5in}{0in}
%\beqn
%\frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} &=& \frac{\oneoversqrt{\sigsq_\mu \parens{\frac{n_{\ell_L}}{\sigsq} + 1}} \oneoversqrt{\sigsq_\mu \parens{\frac{n_{\ell_R}}{\sigsq} + 1}} ~~\exp{\dfrac{n_{\ell_L}^2 \Rbar_{\ell_L}^2 \sigsq_\mu}{2\parens{\sigma^4 + n_{\ell_L}\sigsq \sigsq_\mu}}} ~~\exp{\dfrac{n_{\ell_R}^2 \Rbar_{\ell_R}^2 \sigsq_\mu}{2\parens{\sigma^4 + n_{\ell_R}\sigsq \sigsq_\mu}}}}{\oneoversqrt{\sigsq_\mu \parens{\frac{n_\ell}{\sigsq} + 1}} ~~\exp{\dfrac{n_\ell^2 \Rbar_\ell^2 \sigsq_\mu}{2\parens{\sigma^4 + n_\ell\sigsq \sigsq_\mu}}}} \\
%&=& \sqrt{\frac{\sigsq \parens{n_\ell + \sigsq}}{\sigsq_\mu \parens{n_{\ell_L} + \sigsq}\parens{n_{\ell_R} + \sigsq}}} ~~\exp{\frac{\sigsq_\mu}{2\sigsq} \parens{\frac{\overbrace{n_{\ell_L}^2 \Rbar_{\ell_L}^2}}{\sigsq + n_{\ell_L}\sigsq_\mu} + \frac{\overbrace{n_{\ell_R}^2 \Rbar_{\ell_R}^2}}{\sigsq + n_{\ell_R}\sigsq_\mu} - \frac{\overbrace{n_\ell^2 \Rbar_\ell^2}}{\sigsq + n_\ell \sigsq_\mu}}} \\
%\eeqn
%\end{changemargin}

%Note how the ratio is only a function of the $\sum R_i$'s which makes calculations efficient.

%Let's think about what's really driving this ratio. Consider $\sigsq = \sigsqmu = 1$ and $n_\ell = 10$ with an equal split in left and right daughter leaves. That would mean that the ratio would be:
%
%\beqn
%\sqrt{\frac{11}{36}}~~\exp{\half \parens{\frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell_L, i}}}{6} + \frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell_R, i}}}{6} - \frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell, i}}}{11}}} \\
%\eeqn



\subsubsection*{Strategy \#3 --- Completing the Square}

Recall that:

\begin{changemargin}{-0.5in}{0in}
\beqn
\prob{\Rlonetonl | \mu, \sigsq} &=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \mu}} \\
&=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \parens{n_\ell \squared{\Rbar_\ell - \mu} + \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}}} \\
&=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}} ~\exp{-\frac{n_\ell}{2\sigsq} \squared{\Rbar_\ell - \mu}} \\
&=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}} ~\exp{-\frac{n_\ell}{2\sigsq} \parens{\Rbar_\ell^2 - 2\Rbar_\ell \mu + \musq}} \\
\eeqn
\end{changemargin}

which means that:

\beqn
\cprob{\Rlonetonl}{\sigsq} &=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}} \times \\
&& \myint{\mu}{\reals}{}{\exp{-\frac{n_\ell}{2\sigsq} \parens{\Rbar_\ell^2 - 2\Rbar_\ell \mu + \musq}}\oneoversqrt{2\pi\sigsq_\mu}\exp{-\oneover{2\sigsqmu} \musq}} \\
&=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}} \times \\
&& \oneoversqrt{2\pi\sigsq_\mu} \myint{\mu}{\reals}{}{\exp{-\frac{n_\ell}{2\sigsq} \parens{\Rbar_\ell^2 - 2\Rbar_\ell \mu + \musq} -\oneover{2\sigsqmu} \musq}} \\
&=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}} \times \\
&& \oneoversqrt{2\pi\sigsq_\mu} \myint{\mu}{\reals}{}{\exp{\underbrace{\parens{-\frac{n_\ell}{2\sigsq} - \oneover{2\sigsqmu}}\musq + \parens{\frac{\Rbar_\ell n_\ell}{\sigsq}} \mu} - \frac{n_\ell \Rbar_\ell^2}{2\sigsq}}} \\
\eeqn

Now let's complete the square of the underbraced above. We want it to look like $c\squared{d-\mu} = c\musq - 2cd\mu + cd^2$. Already we have $c$, the factor multiplying $\musq$, so then $d$ becomes (using the $\mu$ term above):

\beqn
d = \frac{\Rbar_\ell n_\ell}{-2c\sigsq} = \frac{\Rbar_\ell n_\ell}{-2 \parens{-\frac{n_\ell}{2\sigsq} - \oneover{2\sigsqmu}} \sigsq} = \frac{\Rbar_\ell n_\ell}{n_\ell + \frac{\sigsq}{\sigsqmu}}
\eeqn

and the final $cd^2$ term becomes:

\beqn
cd^2 = c \squaredfrac{\Rbar_\ell n_\ell}{-2c\sigsq} = \frac{\Rbar_\ell^2 n_\ell^2}{4c \sigma^4} =  \frac{\Rbar_\ell^2 n_\ell^2}{4\parens{-\frac{n_\ell}{2\sigsq} - \oneover{2\sigsqmu}} \sigma^4} = \frac{\Rbar_\ell^2 n_\ell^2}{-2\parens{\frac{n_\ell}{\sigsq} + \oneover{\sigsqmu}} \sigma^4} = \frac{\Rbar_\ell^2 n_\ell^2}{-2\sigsq\parens{n_\ell + \frac{\sigsq}{\sigsqmu}}}
\eeqn

Returning to our margined expression, we subtract and add the $cd^2$ term inside the exponentiation:

\small
\begin{changemargin}{-0.5in}{0in}
\beqn
\cprob{\Rlonetonl}{\sigsq} &=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}} \oneoversqrt{2\pi\sigsq_\mu}\times \\
&&  \myint{\mu}{\reals}{}{\exp{\underbrace{\parens{-\frac{n_\ell}{2\sigsq} - \oneover{2\sigsqmu}}\musq + \parens{\frac{\Rbar_\ell}{\sigsq}} \mu - \frac{\Rbar_\ell^2 n_\ell^2}{2\sigsq\parens{n_\ell + \frac{\sigsq}{\sigsqmu}}}} + \frac{\Rbar_\ell^2 n_\ell^2}{2\sigsq\parens{n_\ell + \frac{\sigsq}{\sigsqmu}}} - \frac{n_\ell \Rbar_\ell^2}{2\sigsq}} } \\
\eeqn
\end{changemargin}
\normalsize

So the underbraced is the ``square'' and we pull the last two terms out (because they are not functions of $\mu$) to arrive at:

\begin{changemargin}{-0.5in}{0in}
\beqn
\cprob{\Rlonetonl}{\sigsq} &=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}} \oneoversqrt{2\pi\sigsq_\mu} \times \\
&& ~\exp{\frac{\Rbar_\ell^2 n_\ell^2}{2\sigsq\parens{n_\ell + \frac{\sigsq}{\sigsqmu}}}- \frac{n_\ell \Rbar_\ell^2}{2\sigsq}} \myint{\mu}{\reals}{}{\exp{\parens{\underbrace{-\frac{n_\ell}{2\sigsq} - \oneover{2\sigsqmu}}}\squared{\mu - \frac{\Rbar_\ell}{n_\ell + \frac{\sigsq}{\sigsqmu}}}}} \\
\eeqn
\end{changemargin}

Note how the integral is now the Gaussian integral with the $-\oneover{2\sigsq_*}$ parameter underbraced. Hence it will integrate to $\sqrt{2\pi\sigsq_*}$ and the expression becomes:

\begin{changemargin}{-0.5in}{0in}
\beqn
\cprob{\Rlonetonl}{\sigsq} &=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} ~\exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}} \times \\
&& \oneoversqrt{2\pi\sigsq_\mu} ~\exp{\frac{\Rbar_\ell^2 n_\ell^2}{2\sigsq\parens{n_\ell + \frac{\sigsq}{\sigsqmu}}}- \frac{n_\ell \Rbar_\ell^2}{2\sigsq}} \sqrt{2\pi \oneover{\frac{n_\ell}{\sigsq} + \oneover{\sigsqmu}}} \\
&=& \oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}} \sqrt{ \frac{\sigsq}{\sigsq + n_\ell \sigsqmu}}  \times \\
&& \exp{-\oneover{2\sigsq} \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}} \exp{\frac{\Rbar_\ell^2 n_\ell^2}{2\sigsq\parens{n_\ell + \frac{\sigsq}{\sigsqmu}}}- \frac{n_\ell \Rbar_\ell^2}{2\sigsq}} \\
\eeqn
\end{changemargin}

So now we're ready to calculate the likelihood ratio.

\begin{changemargin}{-0.75in}{0in}
\beqn
&& \frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} \\
&=& \frac{\cprob{\RLlonetonlL}{\sigsq} \cprob{\RRlonetonlR}{\sigsq}}{\cprob{\Rlonetonl}{\sigsq}} \\
&=& \frac{\cancel{\oneover{\tothepow{2\pi\sigsq}{n_{\ell_L} / 2}}} ~ \exp{\frac{\Rbar_{\ell_L}^2 n_\ell^2}{2\sigsq\parens{n_{\ell_L} + \frac{\sigsq}{\sigsqmu}}}- \frac{n_{\ell_L} \Rbar_{\ell_L}^2 }{2\sigsq}} \sqrt{ \frac{\cancel{\sigsq}}{\sigsq + n_{\ell_L} \sigsqmu}} \cancel{\oneover{\tothepow{2\pi\sigsq}{n_{\ell_R} / 2}}} ~ \exp{\frac{\Rbar_{\ell_R}^2 n_\ell^2}{2\sigsq\parens{n_{\ell_R} + \frac{\sigsq}{\sigsqmu}}}- \frac{n_{\ell_R} \Rbar_{\ell_R}^2}{2\sigsq}} \sqrt{ \frac{\sigsq}{\sigsq + n_{\ell_R} \sigsqmu}} }{\cancel{\oneover{\tothepow{2\pi\sigsq}{n_\ell / 2}}} ~ \exp{\frac{\Rbar_\ell^2 n_\ell^2}{2\sigsq\parens{n_\ell + \frac{\sigsq}{\sigsqmu}}}- \frac{n_\ell \Rbar_\ell^2}{2\sigsq}} \sqrt{ \frac{\cancel{\sigsq}}{\sigsq + n_\ell \sigsqmu}}} \times \\
&& \exp{\underbrace{-\oneover{2\sigsq}\parens{\sum_{i=1}^{n_{\ell_L}} \squared{R_{\ell_L, i} - \Rbar_{\ell_L}} + \sum_{i=1}^{n_{\ell_R}} \squared{R_{\ell_R, i} - \Rbar_{\ell,R}} - \sum_{i=1}^{n_\ell} \squared{R_{\ell_i} - \Rbar_\ell}}}}
\eeqn
\end{changemargin}

We've calculate the underbraced before in the strategy \#1 section. We now substitute, factor, and collect terms:

\beqn
&=& \underbrace{\sqrt{\frac{\sigsq \parens{\sigsq + n_\ell \sigsqmu}}{\parens{\sigsq + n_{\ell_L} \sigsqmu}\parens{\sigsq + n_{\ell_R} \sigsqmu}}}}_c  \times \\ 
&& \exp{\oneover{2\sigsq} \parens{\frac{\Rbar_{\ell_L}^2 n_{\ell_L}^2}{\parens{n_{\ell_L} + \frac{\sigsq}{\sigsqmu}} }- n_{\ell_L}\Rbar_{\ell_L}^2 + \frac{\Rbar_{\ell_R}^2 n_{\ell_R}^2}{\parens{n_{\ell_R} + \frac{\sigsq}{\sigsqmu}}}- n_{\ell_R}\Rbar_{\ell_R}^2 - \frac{\Rbar_{\ell}^2 n_\ell^2}{\parens{n_{\ell} + \frac{\sigsq}{\sigsqmu}}} + n_\ell\Rbar_\ell^2}} \times \\
&& \exp{\oneover{2\sigsq} \parens{n_{\ell_L}\Rbar_{\ell_L}^2 + n_{\ell}\Rbar_{\ell}^2 - n_{\ell}\Rbar_{\ell}^2}} \\
&=& c~\exp{\frac{\sigsqmu}{2\sigsq} \parens{\parens{\frac{n_{\ell_L}^2}{\sigsqmu n_{\ell_L} +\sigsq}} \Rbar_{\ell_L}^2 + \parens{\frac{n_{\ell_R}^2}{\sigsqmu n_{\ell_R} + \sigsq}} \Rbar_{\ell_R}^2 - \parens{\frac{n_\ell^2}{\sigsqmu n_{\ell} + \sigsq}} \Rbar_{\ell}^2}} \\
&=& \sqrt{\frac{\sigsq \parens{\sigsq + n_\ell \sigsqmu}}{\parens{\sigsq + n_{\ell_L} \sigsqmu}\parens{\sigsq + n_{\ell_R} \sigsqmu}}}~~\exp{\frac{\sigsq_\mu}{2\sigsq} \parens{\frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell_L, i}}}{\sigsq + n_{\ell_L}\sigsq_\mu} + \frac{\squared{\sum_{i=1}^{n_{\ell_R}} R_{\ell_R, i}}}{\sigsq + n_{\ell_R}\sigsq_\mu} - \frac{\squared{\sum_{i=1}^{n_{\ell}} R_{\ell, i}}}{\sigsq + n_\ell \sigsq_\mu}}} \\
\eeqn

We've now computed this using three different methods, so it must be correct?\\

In log form, this becomes:

\beqn
&& \half \parens{\natlog{\sigsq} +\natlog{\sigsq + n_\ell \sigsqmu} -\natlog{\sigsq + n_{\ell_L} \sigsqmu} - \natlog{\sigsq + n_{\ell_R} \sigsqmu}} + \\
&& \frac{\sigsq_\mu}{2\sigsq} \parens{\frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell_L, i}}}{\sigsq + n_{\ell_L}\sigsq_\mu} + \frac{\squared{\sum_{i=1}^{n_{\ell_R}} R_{\ell_R, i}}}{\sigsq + n_{\ell_R}\sigsq_\mu} - \frac{\squared{\sum_{i=1}^{n_{\ell}} R_{\ell, i}}}{\sigsq + n_\ell \sigsq_\mu}}
\eeqn

\subsubsection*{Tree Structure Ratio}

Remember the prior form on trees: 

\beqn
\prob{T} &=& \prod_{\eta \in \Hleaves} \parens{1 - \prob{\text{splitting~} \eta}}  \prod_{\eta \in \Hint} \prob{\text{splitting~} \eta} \prod_{\eta \in \Hint} \prob{\text{rule~} \eta} \\
\eeqn

Remember from CGM98 that the probability of splitting on a given node $\eta$ is driven by two hyperparameters, $\alpha$ and $\beta$ in the following way where $d_\eta$ is the depth of the $\eta$ node:

\beqn
\prob{\text{splitting~} \eta} = \frac{\alpha}{\tothepow{1 + d_\eta}{\beta}}
\eeqn

The probability of assigning the specific rule, $\prob{\text{rule~} \eta}$, is just picking from all available attributes and then from all available unique split points so $\oneover{p_\eta}\oneover{n_\eta}$. Once again, the proposal tree differs from the original tree by only the $\ell$th node in the original becoming the $\ell_L$ and $\ell_R$ nodes in the proposal. This simplifies the tree structure ratio to just:

\beqn
\frac{\prob{T^*}}{\prob{T}} &=& \frac{\parens{1 - \prob{\text{splitting~} \eta_L}} \parens{1 - \prob{\text{splitting~} \eta_R}} \prob{\text{splitting~} \eta} \prob{\text{rule~} \eta}}{\parens{1 - \prob{\text{splitting~} \eta}}}\\
&=& \frac{\parens{1 - \dfrac{\alpha}{\tothepow{1 + d_{\eta_L}}{\beta}}}\parens{1 - \dfrac{\alpha}{\tothepow{1 + d_{\eta_R}}{\beta}}} \dfrac{\alpha}{\tothepow{1 + d_{\eta}}{\beta}} \doneover{p_\eta}\doneover{n_\eta}}{1 -\dfrac{\alpha}{\tothepow{1 + d_{\eta}}{\beta}}} \\
&=& \alpha \frac{\squared{1 - \frac{\alpha}{\tothepow{2 + d_\eta}{\beta}}}}{\parens{\tothepow{1+d_\eta}{\beta} - \alpha} p_\eta n_\eta}
\eeqn

With the last line following from the fact that the depth of the child nodes is just the depth of the parent node incremented by 1. In log form this becomes:

\beqn
&&\natlog{\alpha} + 2 \natlog{1 - \frac{\alpha}{\tothepow{2 + d_\eta}{\beta}}} - \natlog{\tothepow{1+d_\eta}{\beta} - \alpha} - \natlog{p_\eta} - \natlog{n_\eta}
\eeqn

Now we have a way of calculating $r$ for grow proposals by multiplying all three above results.\\

\subsection*{Prune Proposal}

\subsubsection*{Transition Ratio}

For prune proposals, we move in the opposite direction.  We need to hack off a node:

\beqn
\prob{T \rightarrow T^*} &=& \prob{\text{PRUNE}} \prob{\text{selecting the $\ell$th node to prune from}} \\
&=& \prob{\text{PRUNE}}\oneover{w_2}
\eeqn

To go the opposite direction, we need to make sure we grow the exact same node so $p_{adj}, n_{adj}$ need to be calculated based on whatever the $\ell$th node originally was:

\beqn
\prob{T^* \rightarrow T} &=& \prob{\text{GROW}} \prob{\text{selecting the $\ell$th node to grow from}} \times \\
&& \prob{\text{selecting the original attribute to split on}} \times \\
&& \prob{\text{selecting the original value to split on}} \\
&=& \prob{\text{GROW}} \oneover{b-1} \oneover{p_{adj}} \oneover{n_{adj}}
\eeqn

We're using $b-1$ here because the proposed tree has one less terminal node due to the pruning than the original tree $T$ had.\\

Thus, the transition ratio becomes:

\beqn
\frac{\prob{T^* \rightarrow T}}{\prob{T \rightarrow T^*}} = \frac{\prob{\text{GROW}}  \oneover{b-1}\oneover{p_{adj}} \oneover{n_{adj}}}{ \prob{\text{PRUNE}}\oneover{w_2}} = \cancelto{?}{\frac{\prob{\text{GROW}}}{\prob{\text{PRUNE}}}} \frac{w_2}{(b-1) p_{adj} n_{adj}}
\eeqn

and in log form this becomes:

\beqn
\natlog{w_2} - \natlog{b - 1} - \natlog{p_{adj}} - \natlog{n_{adj}}
\eeqn

Once again, we need to bookkeep and make sure we can actually prune this node. If it's just a root node, then we can't even consider this step at all. Otherwise, they'll cancel.

\subsubsection*{Likelihood Ratio}

It is pretty obvious this is the inverse of the grow step's likelihood ratio. Now the tree proposal has just one collapsed node where the original has a left and right component:

\begin{changemargin}{-0.5in}{0in}
\beqn
&& \frac{\cprob{\R}{T^*, \sigsq}}{\cprob{\R}{T, \sigsq}} \\
&=& \inverse{\sqrt{\frac{\sigsq \parens{\sigsq + n_\ell \sigsqmu}}{\parens{\sigsq + n_{\ell_L} \sigsqmu}\parens{\sigsq + n_{\ell_R} \sigsqmu}}}~~\exp{\frac{\sigsq_\mu}{2\sigsq} \parens{\frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell_L, i}}}{\sigsq + n_{\ell_L}\sigsq_\mu} + \frac{\squared{\sum_{i=1}^{n_{\ell_R}} R_{\ell_R, i}}}{\sigsq + n_{\ell_R}\sigsq_\mu} - \frac{\squared{\sum_{i=1}^{n_{\ell}} R_{\ell, i}}}{\sigsq + n_\ell \sigsq_\mu}}}} \\
&=& \sqrt{\frac{\parens{\sigsq + n_{\ell_L} \sigsqmu}\parens{\sigsq + n_{\ell_R} \sigsqmu}}{\sigsq \parens{\sigsq + n_\ell \sigsqmu}}} ~~\exp{\frac{\sigsq_\mu}{2\sigsq} \parens{\frac{\squared{\sum_{i=1}^{n_{\ell}} R_{\ell, i}}}{\sigsq + n_\ell \sigsq_\mu} - \frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell_L, i}}}{\sigsq + n_{\ell_L}\sigsq_\mu} - \frac{\squared{\sum_{i=1}^{n_{\ell_R}} R_{\ell_R, i}}}{\sigsq + n_{\ell_R}\sigsq_\mu}}} \\
\eeqn
\end{changemargin}

and in log form:

\beqn
&& \half \parens{\natlog{\sigsq + n_{\ell_L} \sigsqmu} + \natlog{\sigsq + n_{\ell_R} \sigsqmu} - \natlog{\sigsq} - \natlog{\sigsq + n_\ell \sigsqmu} } + \\
&& \frac{\sigsq_\mu}{2\sigsq} \parens{\frac{\squared{\sum_{i=1}^{n_{\ell_L}} R_{\ell_L, i}}}{\sigsq + n_{\ell_L}\sigsq_\mu} + \frac{\squared{\sum_{i=1}^{n_{\ell_R}} R_{\ell_R, i}}}{\sigsq + n_{\ell_R}\sigsq_\mu} - \frac{\squared{\sum_{i=1}^{n_{\ell}} R_{\ell, i}}}{\sigsq + n_\ell \sigsq_\mu}}
\eeqn

\subsubsection*{Tree Structure Ratio}

It is also clear this is just the inverse of the tree structure ratio for the grow step.

\beqn
\frac{\prob{T^*}}{\prob{T}} &=& \inverse{\alpha \frac{\squared{1 - \frac{\alpha}{\tothepow{2 + d_\eta}{\beta}}}}{\parens{\tothepow{1+d_\eta}{\beta} - \alpha} p_\eta n_\eta}}\\
&=& \frac{\parens{\tothepow{1+d_\eta}{\beta} - \alpha} p_\eta n_\eta}{\alpha\squared{1 - \frac{\alpha}{\tothepow{2 + d_\eta}{\beta}}}}
\eeqn


In log form this becomes:

\beqn
\natlog{\tothepow{1+d_\eta}{\beta} - \alpha} + \natlog{p_\eta} + \natlog{n_\eta} -\natlog{\alpha} - 2 \natlog{1 - \frac{\alpha}{\tothepow{2 + d_\eta}{\beta}}} 
\eeqn


\subsection*{Change and Swap Proposals}

Due to the complexity of the bookkeeping, we do not consider these steps.

\subsection*{Implementation Details}

We use what we have above to calculate $r$ for grow and prune steps. In practice, we just take the log to avoid numerical problems. \\

One thing we would like to estimate is the likelihood of the trees over the lifetime of the Gibbs sampler.


\end{document}





































%The change proposal is quite different from the grow and prune. This is taking an internal node (any of the $v$ internal nodes where the root itself is included) and changing the rule by switching both the split attribute and the split rule value.\\
%
%Consider $\prob{T \rightarrow T^*} = \cprob{T^*}{T}$ during a change:
%
%\beqn
%\cprob{T^*}{T} &=& \prob{\text{CHANGE}} \prob{\text{selecting the $\ell$th node to change}} \times \\
%&& \prob{\text{selecting the $j$th attribute to split on}} \times \\
%&& \prob{\text{selecting the $i$th value to split on}} \\
%\eeqn
%
%Then going backwards have the same thing but we need to get back to the original node:
%
%\beqn
%\cprob{T}{T^*} &=& \prob{\text{CHANGE}} \prob{\text{selecting the $\ell$th node to change}} \times \\
%&& \prob{\text{selecting the original attribute to split on}} \times \\
%&& \prob{\text{selecting the original value to split on}} \\
%\eeqn
%
%The transition ratio will have many cancellations. The probability of picking a change proposal is obviously the same. The probability of picking the $\ell$th node is the same. Since there are only a certain number of attributes available to the $\ell$th node, that will be the same. This leaves us with:
%
%\beqn
%\frac{\cprob{T}{T^*}}{\cprob{T^*}{T}} = \frac{n_{\text{original}}}{n_{\text{proposal}}}
%\eeqn
%
%where $n_{\text{original}}$ is the number of split points available if the $\ell$th node's split attribute is the original attribute and $n_{\text{proposal}}$ is the number of split points available if the proposal split attribute is used instead.
%
%We now move on to the proportional likelihood ratio. We note that if we make a change, all daughter nodes are affected all the way down the lineage. Hence, we need to compare the likelihoods of all the daughter nodes underneath the $\ell$th node. There is no clean way to denote this, so I just spell it out:
%
%\beqn
%\frac{\cprob{R}{T^*, \sigsq}}{\cprob{R}{T, \sigsq}} = \frac{\prod_{\ell^* \in \text{all leaf nodes under the $\ell$th node}} { \abss{\bSigma_{\ell^*}}^{-\half} \exp{-\half R_{\ell^*}^\top \bSigmainv_{\ell^*} R_{\ell^*}}}}{\prod_{\ell \in \text{all leaf nodes under the $\ell$th node}} { \abss{\bSigma_\ell}^{-\half} \exp{-\half R_\ell^\top \bSigmainv_\ell R_\ell}}}
%\eeqn
%
%I leave out the $2\pi^{n_\cdot}$ terms because the total number $n$ at any given node will have to remain the same regardless as how it's apportioned in the nodes below it. \\
%
%\textbf{Question}: how is $R_\ell$ defined if the node is empty? \\
%
%Now we move on to the tree structure ratio. Recall that:
%
%\beqn
%\prob{T^*} &=& \prod_{\eta \in H} \prob{\text{splitting~} \eta} \prod_{\eta \in H} \prob{\text{assigning a rule to~} \eta} \\
%\eeqn
%
%Since both the proposal and original trees have the same structure, we can ignore the $\prob{\text{splitting~} \eta}$ terms when we form the ratio. And once again, only the nodes below the changed node need to be considered since all other nodes are the same for both the proposal and original. Thus we're left with:
%
%\beqn
%\frac{\prob{T^*}}{\prob{T}} = \frac{\prod_{h \in \text{all nodes under $\eta$}} \prob{\text{assigning a rule to~} h ~|~ T^*}}{\prod_{h \in \text{all nodes under $\eta$}} \prob{\text{assigning a rule to~} h ~|~ T}} \equalsquestion \frac{\prob{\text{assigning a rule to~}  \eta ~|~ T^*}}{\prob{\text{assigning a rule to~} \eta ~|~ T}}
%\eeqn
%
%The question is does is the $\equalsquestion$ above a true equality? I doubt it. Let's say your original split is $x_{37} < 89.2$ and the proposed change split is $x_{49} < 23.8$. It's possible that other $x_{49}$ rules are present underneath node $\eta$. These rule assignment probabilities will now change because the number of available split values will change. \\
%
%\textbf{Question}: what happens if the proposed change is a minimum value on an attribute, and there are nodes below with the same split attribute, that will force the numerator to be zero and the step should always be rejected? \\
%
%It seems that change steps are quite complicated when there is all this bookkeeping. This may be the reason Rob's new BART implementation only does GROWs and PRUNEs. \\
%
%I really would like to nail down this spec before I go ahead and implement these features, so if both of you could comment, that would be fantastic.
